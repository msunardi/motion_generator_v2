Note: 3rd retrain

Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2552 - acc: 0.8801 - val_loss: 0.2456 - val_acc: 0.8900
Epoch 2/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2513 - acc: 0.8826 - val_loss: 0.2440 - val_acc: 0.8901
Epoch 3/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2539 - acc: 0.8830 - val_loss: 0.2539 - val_acc: 0.8805
Epoch 4/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.2516 - acc: 0.8820 - val_loss: 0.2297 - val_acc: 0.8940
Epoch 5/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2510 - acc: 0.8830 - val_loss: 0.2283 - val_acc: 0.8966
Epoch 6/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2489 - acc: 0.8842 - val_loss: 0.2335 - val_acc: 0.8969
Epoch 7/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2438 - acc: 0.8857 - val_loss: 0.2611 - val_acc: 0.8675
Epoch 8/20
11450/11450 [==============================] - 36s 3ms/step - loss: 0.2469 - acc: 0.8851 - val_loss: 0.2293 - val_acc: 0.8956
Epoch 9/20
11450/11450 [==============================] - 34s 3ms/step - loss: 0.2447 - acc: 0.8865 - val_loss: 0.2249 - val_acc: 0.8981
Epoch 10/20
11450/11450 [==============================] - 37s 3ms/step - loss: 0.2396 - acc: 0.8867 - val_loss: 0.2263 - val_acc: 0.8993
Epoch 11/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.2420 - acc: 0.8873 - val_loss: 0.2238 - val_acc: 0.8982
Epoch 12/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.2402 - acc: 0.8885 - val_loss: 0.2342 - val_acc: 0.8945
Epoch 13/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.2379 - acc: 0.8890 - val_loss: 0.2254 - val_acc: 0.8979
Epoch 14/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2369 - acc: 0.8891 - val_loss: 0.2208 - val_acc: 0.8981
Epoch 15/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2389 - acc: 0.8898 - val_loss: 0.2162 - val_acc: 0.9001
Epoch 16/20
11450/11450 [==============================] - 29s 2ms/step - loss: 0.2410 - acc: 0.8872 - val_loss: 0.2212 - val_acc: 0.9018
Epoch 17/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2384 - acc: 0.8889 - val_loss: 0.2202 - val_acc: 0.9000
Epoch 18/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2429 - acc: 0.8876 - val_loss: 0.2209 - val_acc: 0.8994
Epoch 19/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2333 - acc: 0.8905 - val_loss: 0.2179 - val_acc: 0.9018
Epoch 20/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2319 - acc: 0.8913 - val_loss: 0.2211 - val_acc: 0.9025


model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_1 (GRU)                  (50, 50, 64)              12672     
_________________________________________________________________
gru_2 (GRU)                  (50, 50, 128)             74112     
_________________________________________________________________
dense_1 (Dense)              (50, 50, 2)               258       
=================================================================
Total params: 87,042
Trainable params: 87,042
Non-trainable params: 0
_________________________________________________________________

sample  actual          predicted                       prediction error (over correct class)
0       1       [0.00646331 0.99353665]         0.006463348865509033
1       0       [0.996956   0.00304397]         0.0030440092086791992
2       0       [0.98312044 0.0168796 ]         0.016879558563232422
3       0       [0.95674545 0.04325463]         0.043254554271698
4       0       [0.9935609  0.00643912]         0.006439089775085449
5       1       [0.00428419 0.9957158 ]         0.004284203052520752
6       1       [0.00173723 0.9982628 ]         0.0017371773719787598
7       1       [0.08867568 0.9113243 ]         0.08867567777633667
8       1       [0.4955121 0.5044879]   **0.49551212787628174**
9       1       [0.03112035 0.9688796 ]         0.0311204195022583
10      0       [0.9881578  0.01184221]         0.011842191219329834
11      1       [0.00426671 0.9957333 ]         0.004266679286956787
12      0       [0.99376774 0.00623233]         0.006232261657714844
13      1       [8.033393e-04 9.991967e-01]     0.0008032917976379395
14      0       [0.7536866 0.2463134]   0.24631339311599731
15      1       [0.00564314 0.9943568 ]         0.005643188953399658
16      0       [0.9986362  0.00136383]         0.0013638138771057129
17      0       [0.998154   0.00184595]         0.001846015453338623
18      1       [5.1405054e-04 9.9948597e-01]   0.0005140304565429688
19      0       [0.98868656 0.01131349]         0.011313438415527344
20      0       [0.98710877 0.01289131]         0.012891232967376709
21      1       [0.00692571 0.9930743 ]         0.006925702095031738
22      0       [0.99761415 0.00238591]         0.002385854721069336
23      1       [0.00688247 0.9931176 ]         0.006882429122924805
24      1       [0.0057637 0.9942363]   0.005763709545135498
25      1       [0.00222607 0.99777395]         0.0022260546684265137
26      1       [0.00170351 0.99829656]         0.0017034411430358887
27      1       [0.00433616 0.9956638 ]         0.004336178302764893
28      1       [0.00283622 0.9971637 ]         0.002836287021636963
29      0       [0.49613234 0.5038676 ]         **0.5038676559925079**
30      0       [0.93885654 0.06114347]         0.061143457889556885
31      1       [0.00650505 0.9934949 ]         0.006505072116851807
32      1       [0.00362991 0.9963701 ]         0.003629922866821289
33      0       [0.9964154  0.00358463]         0.003584623336791992
34      1       [0.00241296 0.9975871 ]         0.0024129152297973633
35      1       [0.00131328 0.99868673]         0.0013132691383361816
36      0       [0.9833017  0.01669823]         0.01669830083847046
37      0       [0.9503869  0.04961306]         0.049613118171691895
38      1       [0.00325076 0.9967493 ]         0.003250718116760254
39      1       [0.00224186 0.9977582 ]         0.002241790294647217
40      0       [0.961542   0.03845795]         0.03845798969268799
41      1       [0.01203744 0.98796254]         0.012037456035614014
42      0       [0.976919   0.02308096]         0.02308100461959839
43      0       [0.9916293  0.00837072]         0.008370697498321533
44      1       [0.00301792 0.99698204]         0.0030179619789123535
45      1       [0.00545989 0.99454015]         0.005459845066070557
46      1       [0.00322943 0.9967706 ]         0.003229379653930664
47      1       [0.01217163 0.9878284 ]         0.012171626091003418
48      1       [0.00329228 0.9967077 ]         0.0032923221588134766
49      1       [0.0011989 0.9988011]   0.001198887825012207

================

Confusion matrix

================

        P       N
P       29      1
N       1       19
Precision: 0.9666666666666667
Recall: 0.9666666666666667

Note: 3rd retrain