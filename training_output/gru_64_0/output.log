Layer (type)                 Output Shape              Param #   
=================================================================
gru_8 (GRU)                  (50, 50, 64)              12672     
_________________________________________________________________
dense_4 (Dense)              (50, 50, 2)               130       
=================================================================
Total params: 12,802
Trainable params: 12,802
Non-trainable params: 0
_________________________________________________________________
None
Train on 21900 samples, validate on 2700 samples
Epoch 1/20
21900/21900 [==============================] - 16s 751us/step - loss: 0.6849 - acc: 0.5592 - val_loss: 0.6804 - val_acc: 0.5689
Epoch 2/20
21900/21900 [==============================] - 16s 739us/step - loss: 0.6725 - acc: 0.5795 - val_loss: 0.6670 - val_acc: 0.6009
Epoch 3/20
21900/21900 [==============================] - 17s 762us/step - loss: 0.6596 - acc: 0.6094 - val_loss: 0.6618 - val_acc: 0.6080
Epoch 4/20
21900/21900 [==============================] - 16s 732us/step - loss: 0.6586 - acc: 0.6147 - val_loss: 0.6574 - val_acc: 0.6150
Epoch 5/20
21900/21900 [==============================] - 16s 730us/step - loss: 0.6568 - acc: 0.6147 - val_loss: 0.6609 - val_acc: 0.6091
Epoch 6/20
21900/21900 [==============================] - 16s 725us/step - loss: 0.6554 - acc: 0.6173 - val_loss: 0.6551 - val_acc: 0.6186
Epoch 7/20
21900/21900 [==============================] - 16s 734us/step - loss: 0.6535 - acc: 0.6194 - val_loss: 0.6591 - val_acc: 0.6106
Epoch 8/20
21900/21900 [==============================] - 17s 777us/step - loss: 0.6522 - acc: 0.6206 - val_loss: 0.6567 - val_acc: 0.6134
Epoch 9/20
21900/21900 [==============================] - 17s 784us/step - loss: 0.6518 - acc: 0.6211 - val_loss: 0.6548 - val_acc: 0.6178
Epoch 10/20
21900/21900 [==============================] - 16s 734us/step - loss: 0.6509 - acc: 0.6224 - val_loss: 0.6531 - val_acc: 0.6178
Epoch 11/20
21900/21900 [==============================] - 16s 742us/step - loss: 0.6477 - acc: 0.6258 - val_loss: 0.6545 - val_acc: 0.6147
Epoch 12/20
21900/21900 [==============================] - 17s 762us/step - loss: 0.6474 - acc: 0.6263 - val_loss: 0.6551 - val_acc: 0.6126
Epoch 13/20
21900/21900 [==============================] - 17s 787us/step - loss: 0.6458 - acc: 0.6276 - val_loss: 0.6529 - val_acc: 0.6184
Epoch 14/20
21900/21900 [==============================] - 17s 775us/step - loss: 0.6447 - acc: 0.6299 - val_loss: 0.6448 - val_acc: 0.6325
Epoch 15/20
21900/21900 [==============================] - 24s 1ms/step - loss: 0.6433 - acc: 0.6308 - val_loss: 0.6463 - val_acc: 0.6269
Epoch 16/20
21900/21900 [==============================] - 17s 761us/step - loss: 0.6420 - acc: 0.6347 - val_loss: 0.6516 - val_acc: 0.6239
Epoch 17/20
21900/21900 [==============================] - 18s 808us/step - loss: 0.6413 - acc: 0.6356 - val_loss: 0.6439 - val_acc: 0.6342
Epoch 18/20
21900/21900 [==============================] - 18s 810us/step - loss: 0.6393 - acc: 0.6369 - val_loss: 0.6418 - val_acc: 0.6339
Epoch 19/20
21900/21900 [==============================] - 21s 953us/step - loss: 0.6369 - acc: 0.6407 - val_loss: 0.6408 - val_acc: 0.6352
Epoch 20/20
21900/21900 [==============================] - 22s 1ms/step - loss: 0.6356 - acc: 0.6412 - val_loss: 0.6447 - val_acc: 0.6364
sample  actual          predicted                       prediction error (over correct class)
0       1       [0.7086588  0.29134116]         **0.7086588442325592**
1       1       [0.2490146  0.75098544]         0.24901455640792847
2       1       [0.5095681 0.4904319]   **0.5095680952072144**
3       1       [0.30259997 0.69740003]         0.3025999665260315
4       1       [0.40760309 0.5923969 ]         **0.40760308504104614**
5       0       [0.6491154 0.3508846]   0.3508846163749695
6       1       [0.22869278 0.77130723]         0.22869277000427246
7       0       [0.5568012  0.44319874]         **0.4431988000869751**
8       1       [0.32666862 0.6733314 ]         0.3266686201095581
9       1       [0.6627474  0.33725262]         **0.6627473831176758**
10      0       [0.54222775 0.45777225]         **0.45777225494384766**
11      0       [0.55176836 0.44823167]         **0.44823163747787476**
12      0       [0.27885038 0.72114956]         **0.7211496233940125**
13      0       [0.31519318 0.6848068 ]         **0.6848068237304688**
14      1       [0.51941246 0.4805875 ]         **0.519412487745285**
15      0       [0.24014144 0.7598586 ]         **0.759858563542366**
16      1       [0.56901807 0.43098187]         **0.5690181255340576**
17      1       [0.5986287 0.4013713]   **0.5986286997795105**
18      0       [0.4519786 0.5480214]   **0.5480214059352875**
19      0       [0.41618437 0.58381563]         **0.5838156342506409**
20      1       [0.627688   0.37231207]         **0.627687931060791**
21      1       [0.35092783 0.6490722 ]         0.35092782974243164
22      1       [0.26010367 0.7398963 ]         0.260103702545166
23      1       [0.23196672 0.7680332 ]         0.2319667935371399
24      1       [0.31016463 0.68983537]         0.3101646304130554
25      1       [0.23211817 0.7678818 ]         0.23211818933486938
26      0       [0.3852558 0.6147441]   **0.6147441864013672**
27      1       [0.41272402 0.587276  ]         **0.41272401809692383**
28      1       [0.22079988 0.7792001 ]         0.22079992294311523
29      0       [0.7171508  0.28284913]         0.28284919261932373
30      1       [0.54716897 0.45283106]         **0.5471689403057098**
31      0       [0.35531738 0.64468265]         **0.6446826159954071**
32      1       [0.26555383 0.7344462 ]         0.2655537724494934
33      1       [0.24524185 0.7547582 ]         0.24524182081222534
34      1       [0.4790286 0.5209714]   **0.479028582572937**
35      1       [0.27528605 0.724714  ]         0.2752860188484192
36      1       [0.24941467 0.7505854 ]         0.2494146227836609
37      1       [0.24144152 0.7585585 ]         0.2414414882659912
38      1       [0.23381697 0.76618296]         0.23381704092025757
39      0       [0.46856162 0.53143835]         **0.5314383804798126**
40      1       [0.348182 0.651818]     0.3481820225715637
41      0       [0.49305072 0.5069493 ]         **0.5069492757320404**
42      1       [0.5216794 0.4783206]   **0.5216794013977051**
43      1       [0.616291   0.38370904]         **0.616290956735611**
44      1       [0.24938841 0.75061154]         0.2493884563446045
45      0       [0.7433754 0.2566246]   0.25662457942962646
46      0       [0.5600206  0.43997943]         **0.4399793744087219**
47      0       [0.60759115 0.39240885]         0.3924088478088379
48      1       [0.40043604 0.59956396]         **0.40043604373931885**
49      0       [0.5906584  0.40934154]         **0.40934157371520996**

================

Confusion matrix

================

        P       N
P       18      14
N       14      4
Precision: 0.5625
Recall: 0.5625

Notes:
- added -1 on walk to remove label from data
- TensorBoard event: 20180214_014157