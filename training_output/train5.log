Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.4300 - acc: 0.7911 - val_loss: 0.3886 - val_acc: 0.8147
Epoch 2/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.4001 - acc: 0.8093 - val_loss: 0.3822 - val_acc: 0.8115
Epoch 3/20
11450/11450 [==============================] - 29s 3ms/step - loss: 0.3732 - acc: 0.8215 - val_loss: 0.3399 - val_acc: 0.8390
Epoch 4/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.3506 - acc: 0.8327 - val_loss: 0.3528 - val_acc: 0.8255
Epoch 5/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.3392 - acc: 0.8395 - val_loss: 0.3336 - val_acc: 0.8436
Epoch 6/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.3282 - acc: 0.8449 - val_loss: 0.3181 - val_acc: 0.8490
Epoch 7/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.3178 - acc: 0.8508 - val_loss: 0.3097 - val_acc: 0.8519
Epoch 8/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.3007 - acc: 0.8578 - val_loss: 0.3301 - val_acc: 0.8524
Epoch 9/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2974 - acc: 0.8593 - val_loss: 0.3432 - val_acc: 0.8282
Epoch 10/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2885 - acc: 0.8639 - val_loss: 0.2943 - val_acc: 0.8649
Epoch 11/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2893 - acc: 0.8645 - val_loss: 0.2900 - val_acc: 0.8622
Epoch 12/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2814 - acc: 0.8685 - val_loss: 0.2683 - val_acc: 0.8727
Epoch 13/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2746 - acc: 0.8712 - val_loss: 0.2733 - val_acc: 0.8708
Epoch 14/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2696 - acc: 0.8739 - val_loss: 0.2775 - val_acc: 0.8713
Epoch 15/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2652 - acc: 0.8760 - val_loss: 0.2795 - val_acc: 0.8682
Epoch 16/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2625 - acc: 0.8779 - val_loss: 0.2578 - val_acc: 0.8786
Epoch 17/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2647 - acc: 0.8769 - val_loss: 0.2569 - val_acc: 0.8781
Epoch 18/20
11450/11450 [==============================] - 29s 3ms/step - loss: 0.2568 - acc: 0.8807 - val_loss: 0.2873 - val_acc: 0.8598
Epoch 19/20
11450/11450 [==============================] - 27s 2ms/step - loss: 0.2544 - acc: 0.8818 - val_loss: 0.2535 - val_acc: 0.8816
Epoch 20/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2527 - acc: 0.8815 - val_loss: 0.2517 - val_acc: 0.8790

model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_14 (GRU)                 (50, 50, 64)              12672     
_________________________________________________________________
gru_15 (GRU)                 (50, 50, 128)             74112     
_________________________________________________________________
dense_8 (Dense)              (50, 50, 2)               258       
=================================================================
Total params: 87,042
Trainable params: 87,042
Non-trainable params: 0
_________________________________________________________________

sample  actual          predicted                       prediction error (over correct class)
0       0       [0.99095166 0.00904837]         0.00904834270477295
1       0       [0.93648547 0.06351449]         0.06351453065872192
2       0       [0.7894871  0.21051286]         0.21051287651062012
3       0       [0.9797932  0.02020678]         0.020206809043884277
4       1       [0.00415414 0.99584585]         0.00415414571762085
5       1       [0.01164727 0.9883527 ]         0.011647284030914307
6       0       [0.9918509  0.00814912]         0.00814908742904663
7       1       [0.00447233 0.9955277 ]         0.004472315311431885
8       1       [0.00416503 0.99583495]         0.004165053367614746
9       0       [0.93406856 0.06593145]         0.06593143939971924
10      1       [0.00976212 0.9902379 ]         0.009762108325958252
11      1       [0.00458497 0.995415  ]         0.0045850276947021484
12      1       [0.01310389 0.9868961 ]         0.013103902339935303
13      0       [0.9922713  0.00772866]         0.007728695869445801
14      1       [0.00484464 0.99515533]         0.00484466552734375
15      0       [0.9809512  0.01904879]         0.01904881000518799
16      1       [0.00757836 0.9924217 ]         0.00757831335067749
17      1       [0.01217272 0.98782736]         0.0121726393699646
18      1       [0.00336996 0.99663   ]         0.0033699870109558105
19      0       [0.9929471  0.00705285]         0.007052898406982422
20      1       [0.00308703 0.99691296]         0.0030870437622070312
21      1       [0.00780652 0.99219346]         0.007806539535522461
22      0       [0.99392235 0.00607771]         0.0060776472091674805
23      0       [0.9895779  0.01042212]         0.010422110557556152
24      0       [0.82548773 0.17451227]         0.17451226711273193
25      0       [0.9160838  0.08391617]         0.08391618728637695
26      1       [0.00295548 0.9970445 ]         0.002955496311187744
27      1       [0.00545903 0.994541  ]         0.005459010601043701
28      0       [0.97381955 0.02618044]         0.0261804461479187
29      0       [0.620541   0.37945908]         0.379459023475647
30      1       [0.01290775 0.98709226]         0.012907743453979492
31      1       [0.01118787 0.9888121 ]         0.011187911033630371
32      1       [0.21888164 0.78111833]         0.21888166666030884
33      1       [0.5661427  0.43385732]         **0.5661426782608032**
34      1       [0.00322286 0.9967771 ]         0.0032228827476501465
35      1       [0.5763267  0.42367333]         **0.5763266682624817**
36      0       [0.5119255  0.48807445]         **0.48807448148727417**
37      1       [0.00911902 0.99088097]         0.009119033813476562
38      1       [0.00418491 0.9958151 ]         0.004184901714324951
39      0       [0.9866952  0.01330482]         0.013304829597473145
40      1       [0.00838515 0.9916148 ]         0.008385181427001953
41      0       [0.97570306 0.02429691]         0.024296939373016357
42      1       [0.01363542 0.9863646 ]         0.013635396957397461
43      0       [0.98965096 0.01034908]         0.010349035263061523
44      0       [0.9853189  0.01468109]         0.014681100845336914
45      1       [0.00360344 0.9963966 ]         0.0036033987998962402
46      1       [0.01720811 0.98279184]         0.01720815896987915
47      1       [0.0056084 0.9943916]   0.00560837984085083
48      0       [0.98294777 0.01705227]         0.01705223321914673
49      0       [0.98839635 0.01160359]         0.01160365343093872

================

Confusion matrix

================

        P       N
P       26      2
N       1       21

On test data:
Precision: 0.9629629629629629
Recall: 0.9285714285714286

Note: 2nd retraining of model with the same dataset