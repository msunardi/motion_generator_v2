_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_9 (GRU)                  (50, 50, 64)              12672     
_________________________________________________________________
gru_10 (GRU)                 (50, 50, 128)             74112     
_________________________________________________________________
dense_5 (Dense)              (50, 50, 2)               258       
=================================================================
Total params: 87,042
Trainable params: 87,042
Non-trainable params: 0
_________________________________________________________________
None
Train on 21900 samples, validate on 2700 samples
Epoch 1/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.6838 - acc: 0.5602 - val_loss: 0.6759 - val_acc: 0.5940
Epoch 2/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.6620 - acc: 0.6079 - val_loss: 0.6653 - val_acc: 0.6018
Epoch 3/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.6551 - acc: 0.6180 - val_loss: 0.6550 - val_acc: 0.6157
Epoch 4/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.6507 - acc: 0.6241 - val_loss: 0.6508 - val_acc: 0.6287
Epoch 5/20
21900/21900 [==============================] - 51s 2ms/step - loss: 0.6464 - acc: 0.6284 - val_loss: 0.6518 - val_acc: 0.6341
Epoch 6/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.6398 - acc: 0.6349 - val_loss: 0.6410 - val_acc: 0.6386
Epoch 7/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.6255 - acc: 0.6495 - val_loss: 0.6070 - val_acc: 0.6664
Epoch 8/20
21900/21900 [==============================] - 51s 2ms/step - loss: 0.5900 - acc: 0.6825 - val_loss: 0.5688 - val_acc: 0.6951
Epoch 9/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.5193 - acc: 0.7369 - val_loss: 0.4425 - val_acc: 0.7908
Epoch 10/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.4342 - acc: 0.7905 - val_loss: 0.3673 - val_acc: 0.8312
Epoch 11/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.3796 - acc: 0.8209 - val_loss: 0.3369 - val_acc: 0.8478
Epoch 12/20
21900/21900 [==============================] - 55s 2ms/step - loss: 0.3392 - acc: 0.8424 - val_loss: 0.5555 - val_acc: 0.7451
Epoch 13/20
21900/21900 [==============================] - 53s 2ms/step - loss: 0.3112 - acc: 0.8561 - val_loss: 0.2886 - val_acc: 0.8691
Epoch 14/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.2923 - acc: 0.8660 - val_loss: 0.2887 - val_acc: 0.8685
Epoch 15/20
21900/21900 [==============================] - 51s 2ms/step - loss: 0.2755 - acc: 0.8746 - val_loss: 0.4438 - val_acc: 0.7887
Epoch 16/20
21900/21900 [==============================] - 53s 2ms/step - loss: 0.2634 - acc: 0.8801 - val_loss: 0.2674 - val_acc: 0.8792
Epoch 17/20
21900/21900 [==============================] - 54s 2ms/step - loss: 0.2545 - acc: 0.8844 - val_loss: 0.2432 - val_acc: 0.8917
Epoch 18/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.2492 - acc: 0.8860 - val_loss: 0.2358 - val_acc: 0.8959
Epoch 19/20
21900/21900 [==============================] - 51s 2ms/step - loss: 0.2463 - acc: 0.8880 - val_loss: 0.2313 - val_acc: 0.8997
Epoch 20/20
21900/21900 [==============================] - 52s 2ms/step - loss: 0.2382 - acc: 0.8919 - val_loss: 0.2332 - val_acc: 0.8958
sample  actual          predicted                       prediction error (over correct class)
0       1       [0.9048432  0.09515673]         **0.904843270778656**
1       1       [0.09487221 0.9051278 ]         0.09487217664718628
2       0       [0.90492374 0.09507626]         0.09507626295089722
3       1       [0.00455236 0.9954476 ]         0.00455242395401001
4       1       [0.01157018 0.9884298 ]         0.011570215225219727
5       0       [0.9930534  0.00694661]         0.0069466233253479
6       1       [0.00780512 0.9921949 ]         0.0078051090240478516
7       1       [0.03743395 0.962566  ]         0.03743398189544678
8       1       [0.00569103 0.99430895]         0.005691051483154297
9       1       [0.01676239 0.9832376 ]         0.016762375831604004
10      0       [0.9678831  0.03211696]         0.03211688995361328
11      1       [0.1560772 0.8439228]   0.15607720613479614
12      0       [0.98516184 0.01483812]         0.014838159084320068
13      1       [0.0052183 0.9947817]   0.005218327045440674
14      1       [0.01010849 0.9898915 ]         0.010108470916748047
15      0       [0.88600576 0.11399423]         0.11399424076080322
16      1       [6.8336463e-04 9.9931669e-01]   0.0006833076477050781
17      0       [9.9904865e-01 9.5129851e-04]   0.0009513497352600098
18      1       [0.0125872 0.9874128]   0.012587189674377441
19      0       [0.9956232  0.00437681]         0.004376828670501709
20      1       [0.009206 0.990794]     0.009205996990203857
21      1       [0.01891659 0.98108333]         0.018916666507720947
22      1       [0.00359169 0.9964083 ]         0.0035917162895202637
23      0       [0.99377525 0.00622469]         0.0062247514724731445
24      1       [0.00969876 0.99030125]         0.009698748588562012
25      1       [0.00394622 0.9960538 ]         0.003946185111999512
26      1       [0.9008943  0.09910565]         **0.9008943513035774**
27      1       [0.01898701 0.981013  ]         0.018986999988555908
28      0       [0.96151847 0.03848152]         0.03848153352737427
29      1       [0.00437327 0.9956267 ]         0.004373311996459961
30      0       [0.5813753 0.4186247]   **0.4186246991157532**
31      1       [0.01027187 0.98972815]         0.010271847248077393
32      0       [0.97668594 0.02331406]         0.023314058780670166
33      1       [0.01295905 0.98704094]         0.012959063053131104
34      1       [0.00536111 0.99463886]         0.00536113977432251
35      0       [0.95785373 0.04214628]         0.042146265506744385
36      1       [0.00671296 0.993287  ]         0.006712973117828369
37      1       [0.00343874 0.9965612 ]         0.0034387707710266113
38      0       [0.89030004 0.10969997]         0.10969996452331543
39      0       [0.9120725  0.08792749]         0.08792752027511597
40      1       [0.00191645 0.9980836 ]         0.0019164085388183594
41      1       [0.00410535 0.9958947 ]         0.004105329513549805
42      1       [0.01033062 0.9896694 ]         0.010330617427825928
43      1       [0.00347021 0.99652976]         0.0034702420234680176
44      0       [0.90024763 0.09975237]         0.09975236654281616
45      1       [0.00297683 0.99702317]         0.002976834774017334
46      0       [0.984838   0.01516193]         0.015161991119384766
47      0       [0.9970499 0.00295  ]   0.0029500722885131836
48      0       [0.9946662 0.0053338]   0.0053337812423706055
49      1       [0.00806619 0.9919338 ]         0.008066177368164062

================

Confusion matrix

================

        P       N
P       30      2
N       1       17
Precision: 0.967741935483871
Recall: 0.9375

#=====
#TEST2
#=====
sample  actual          predicted                       prediction error (over correct class)
0       1       [0.00399836 0.99600166]         0.0039983391761779785
1       1       [0.0052575 0.9947425]   0.0052574872970581055
2       0       [0.93936646 0.0606336 ]         0.06063354015350342
3       1       [0.00611809 0.99388194]         0.006118059158325195
4       0       [0.9948612  0.00513877]         0.005138814449310303
5       1       [0.02102013 0.9789798 ]         0.021020174026489258
6       0       [0.98864496 0.011355  ]         0.011355042457580566
7       1       [0.00507385 0.9949261 ]         0.005073904991149902
8       1       [0.01735211 0.98264796]         0.017352044582366943
9       1       [0.01155956 0.9884404 ]         0.011559605598449707
10      1       [0.00490051 0.9950995 ]         0.004900515079498291
11      1       [0.00496332 0.99503666]         0.004963338375091553
12      1       [0.05144965 0.9485503 ]         0.051449716091156006
13      0       [0.9939991  0.00600089]         0.006000876426696777
14      0       [0.99341184 0.00658815]         0.006588160991668701
15      0       [0.9388447  0.06115531]         0.06115531921386719
16      1       [0.00653153 0.9934684 ]         0.0065315961837768555
17      1       [0.01245305 0.987547  ]         0.012453019618988037
18      0       [0.98942393 0.0105761 ]         0.010576069355010986
19      1       [0.00720464 0.9927954 ]         0.007204592227935791
20      0       [0.9837493  0.01625076]         0.01625072956085205
21      1       [0.008718 0.991282]     0.008718013763427734
22      0       [0.96521366 0.03478638]         0.034786343574523926
23      0       [0.9917802  0.00821973]         0.008219778537750244
24      0       [0.9975871  0.00241287]         0.0024129152297973633
25      0       [0.99066037 0.00933959]         0.009339630603790283
26      0       [0.99388367 0.00611625]         0.006116330623626709
27      1       [0.01526501 0.984735  ]         0.01526498794555664
28      0       [0.97431225 0.02568782]         0.025687754154205322
29      0       [0.9914799  0.00852015]         0.008520126342773438
30      1       [0.00695907 0.9930409 ]         0.006959080696105957
31      0       [0.930371   0.06962906]         0.0696290135383606
32      1       [0.00222936 0.99777067]         0.00222933292388916
33      0       [0.83696365 0.16303638]         0.16303634643554688
34      0       [0.97119844 0.02880155]         0.028801560401916504
35      1       [0.05367659 0.9463234 ]         0.053676605224609375
36      0       [0.99793684 0.00206323]         0.002063155174255371
37      1       [0.27230856 0.7276914 ]         0.2723085880279541
38      0       [0.9952148  0.00478514]         0.00478518009185791
39      1       [0.00649443 0.9935056 ]         0.006494402885437012
40      1       [0.00289677 0.9971033 ]         0.002896726131439209
41      0       [0.9856089  0.01439111]         0.014391124248504639
42      1       [0.84720546 0.1527946 ]         **0.847205400466919**
43      1       [0.28068396 0.71931607]         0.2806839346885681
44      1       [0.00719124 0.9928088 ]         0.007191181182861328
45      0       [0.9565681  0.04343186]         0.043431878089904785
46      1       [0.08346292 0.91653705]         0.08346295356750488
47      1       [0.00426459 0.9957354 ]         0.0042645931243896484
48      1       [0.00547712 0.9945229 ]         0.0054770708084106445
49      0       [0.9159625  0.08403753]         0.08403748273849487

================

Confusion matrix

================

        P       N
P       26      1
N       0       23
Precision: 1.0
Recall: 0.9629629629629629

TensorBoard event: 20180214_015646