model.fit(train_x, train_y,
          batch_size=batch_size, epochs=epochs, shuffle=True,
          validation_data=(validate_x, validate_y))
          
Train on 11450 samples, validate on 1400 samples
Epoch 1/30
11450/11450 [==============================] - 321s 28ms/step - loss: 0.6668 - acc: 0.6005 - val_loss: 0.6677 - val_acc: 0.5943
Epoch 2/30
11450/11450 [==============================] - 317s 28ms/step - loss: 0.6601 - acc: 0.6107 - val_loss: 0.6477 - val_acc: 0.6144
Epoch 3/30
11450/11450 [==============================] - 317s 28ms/step - loss: 0.6549 - acc: 0.6192 - val_loss: 0.6647 - val_acc: 0.6134
Epoch 4/30
11450/11450 [==============================] - 318s 28ms/step - loss: 0.6532 - acc: 0.6225 - val_loss: 0.6389 - val_acc: 0.6381
Epoch 5/30
11450/11450 [==============================] - 320s 28ms/step - loss: 0.6486 - acc: 0.6301 - val_loss: 0.6293 - val_acc: 0.6526
Epoch 6/30
11450/11450 [==============================] - 323s 28ms/step - loss: 0.6385 - acc: 0.6397 - val_loss: 0.6244 - val_acc: 0.6520
Epoch 7/30
11450/11450 [==============================] - 325s 28ms/step - loss: 0.6240 - acc: 0.6500 - val_loss: 0.5943 - val_acc: 0.6735
Epoch 8/30
11450/11450 [==============================] - 325s 28ms/step - loss: 0.6234 - acc: 0.6491 - val_loss: 0.6381 - val_acc: 0.6212
Epoch 9/30
11450/11450 [==============================] - 325s 28ms/step - loss: 0.6209 - acc: 0.6497 - val_loss: 0.6571 - val_acc: 0.6169
Epoch 10/30
11450/11450 [==============================] - 323s 28ms/step - loss: 0.6532 - acc: 0.6204 - val_loss: 0.5855 - val_acc: 0.6833
Epoch 11/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.5994 - acc: 0.6723 - val_loss: 0.5513 - val_acc: 0.7159
Epoch 12/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6273 - acc: 0.6486 - val_loss: 0.6354 - val_acc: 0.6338
Epoch 13/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6627 - acc: 0.6076 - val_loss: 0.6963 - val_acc: 0.5187
Epoch 14/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6956 - acc: 0.5574 - val_loss: 0.6630 - val_acc: 0.6078
Epoch 15/30
11450/11450 [==============================] - 311s 27ms/step - loss: 0.6978 - acc: 0.5508 - val_loss: 0.7324 - val_acc: 0.4418
Epoch 16/30
11450/11450 [==============================] - 311s 27ms/step - loss: 0.6970 - acc: 0.5425 - val_loss: 0.6800 - val_acc: 0.5659
Epoch 17/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6889 - acc: 0.5623 - val_loss: 0.7072 - val_acc: 0.5691
Epoch 18/30
11450/11450 [==============================] - 311s 27ms/step - loss: 0.6995 - acc: 0.5408 - val_loss: 0.7082 - val_acc: 0.4756
Epoch 19/30
11450/11450 [==============================] - 311s 27ms/step - loss: 0.6989 - acc: 0.5385 - val_loss: 0.7384 - val_acc: 0.5693
Epoch 20/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6969 - acc: 0.5446 - val_loss: 0.6953 - val_acc: 0.5711
Epoch 21/30
11450/11450 [==============================] - 311s 27ms/step - loss: 0.6970 - acc: 0.5422 - val_loss: 0.6858 - val_acc: 0.5661
Epoch 22/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6986 - acc: 0.5382 - val_loss: 0.6875 - val_acc: 0.5693
Epoch 23/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6978 - acc: 0.5401 - val_loss: 0.6992 - val_acc: 0.5037
Epoch 24/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6981 - acc: 0.5414 - val_loss: 0.6829 - val_acc: 0.5746
Epoch 25/30
11450/11450 [==============================] - 314s 27ms/step - loss: 0.6952 - acc: 0.5434 - val_loss: 0.6917 - val_acc: 0.5333
Epoch 26/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6961 - acc: 0.5432 - val_loss: 0.6900 - val_acc: 0.5688
Epoch 27/30
11450/11450 [==============================] - 311s 27ms/step - loss: 0.6974 - acc: 0.5395 - val_loss: 0.6909 - val_acc: 0.5680
Epoch 28/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6975 - acc: 0.5369 - val_loss: 0.6899 - val_acc: 0.5357
Epoch 29/30
11450/11450 [==============================] - 313s 27ms/step - loss: 0.6976 - acc: 0.5410 - val_loss: 0.6967 - val_acc: 0.5415
Epoch 30/30
11450/11450 [==============================] - 312s 27ms/step - loss: 0.6956 - acc: 0.5453 - val_loss: 0.6934 - val_acc: 0.5388

model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_3 (GRU)                  (50, 50, 512)             789504    
_________________________________________________________________
gru_4 (GRU)                  (50, 50, 512)             1574400   
_________________________________________________________________
dense_2 (Dense)              (50, 50, 2)               1026      
=================================================================
Total params: 2,364,930
Trainable params: 2,364,930
Non-trainable params: 0
_________________________________________________________________

# TEST
sample  actual          predicted                       prediction error (over correct class)
0       1       [0.42503548 0.5749645 ]         **0.4250354766845703**
1       1       [0.46953946 0.5304605 ]         **0.4695395231246948**
2       1       [0.41617727 0.5838227 ]         **0.41617727279663086**
3       1       [0.43353412 0.56646585]         **0.4335341453552246**
4       1       [0.38464972 0.6153503 ]         0.38464969396591187
5       1       [0.5218528  0.47814718]         **0.5218528211116791**
6       1       [0.36567926 0.63432074]         0.3656792640686035
7       1       [0.3787282 0.6212718]   0.3787282109260559
8       1       [0.33957052 0.66042954]         0.33957046270370483
9       1       [0.34467456 0.6553254 ]         0.34467458724975586
10      1       [0.4823542 0.5176458]   **0.48235422372817993**
11      1       [0.48553056 0.5144695 ]         **0.4855304956436157**
12      1       [0.66260266 0.3373973 ]         **0.6626026928424835**
13      1       [0.45830593 0.54169405]         **0.4583059549331665**
14      1       [0.4011724 0.5988276]   **0.401172399520874**
15      1       [0.38131338 0.61868656]         0.3813134431838989
16      1       [0.44941652 0.5505835 ]         **0.44941651821136475**
17      1       [0.35882837 0.64117163]         0.3588283658027649
18      1       [0.32953447 0.6704655 ]         0.32953447103500366
19      1       [0.5027431  0.49725685]         **0.5027431547641754**
20      1       [0.50252116 0.4974788 ]         **0.5025211870670319**
21      1       [0.49256006 0.50744   ]         **0.4925600290298462**
22      1       [0.4073983 0.5926017]   **0.4073982834815979**
23      1       [0.42840835 0.5715916 ]         **0.4284083843231201**
24      1       [0.347165 0.652835]     0.34716498851776123
25      1       [0.4810067 0.5189933]   **0.4810066819190979**
26      1       [0.4862095 0.5137905]   **0.486209511756897**
27      1       [0.44798607 0.55201393]         **0.44798606634140015**
28      1       [0.35942996 0.64057004]         0.3594299554824829
29      1       [0.49856564 0.5014344 ]         **0.4985656142234802**
30      1       [0.495526   0.50447404]         **0.49552595615386963**
31      1       [0.36633176 0.63366824]         0.3663317561149597
32      1       [0.3512968 0.6487032]   0.3512967824935913
33      1       [0.5065439  0.49345613]         **0.5065438747406006**
34      1       [0.36329478 0.6367052 ]         0.363294780254364
35      1       [0.3794674  0.62053263]         0.3794673681259155
36      1       [0.53954965 0.4604504 ]         **0.5395495891571045**
37      1       [0.5338165  0.46618348]         **0.5338165163993835**
38      1       [0.422226 0.577774]     **0.4222260117530823**
39      1       [0.4400016 0.5599984]   **0.44000160694122314**
40      1       [0.46219185 0.5378082 ]         **0.4621918201446533**
41      1       [0.37522522 0.6247748 ]         0.3752251863479614
42      1       [0.5561748  0.44382522]         **0.5561747848987579**
43      1       [0.43821543 0.56178457]         **0.438215434551239**
44      1       [0.5468976  0.45310244]         **0.5468975603580475**
45      1       [0.3775019 0.6224981]   0.377501904964447
46      1       [0.48565543 0.5143446 ]         **0.48565542697906494**
47      1       [0.48003352 0.5199665 ]         **0.4800335168838501**
48      1       [0.4206379 0.5793621]   **0.42063790559768677**
49      1       [0.5506425  0.44935754]         **0.5506424605846405**

================

Confusion matrix

================

        P       N
P       16      34
N       0       0
