_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_11 (GRU)                 (50, 50, 64)              12672     
_________________________________________________________________
gru_12 (GRU)                 (50, 50, 128)             74112     
_________________________________________________________________
gru_13 (GRU)                 (50, 50, 256)             295680    
_________________________________________________________________
dense_6 (Dense)              (50, 50, 2)               514       
=================================================================
Total params: 382,978
Trainable params: 382,978
Non-trainable params: 0
_________________________________________________________________
None
Train on 21900 samples, validate on 2700 samples
Epoch 1/20
21900/21900 [==============================] - 140s 6ms/step - loss: 0.6823 - acc: 0.5661 - val_loss: 0.6693 - val_acc: 0.5856
Epoch 2/20
21900/21900 [==============================] - 143s 7ms/step - loss: 0.6648 - acc: 0.6029 - val_loss: 0.6705 - val_acc: 0.5852
Epoch 3/20
21900/21900 [==============================] - 139s 6ms/step - loss: 0.6550 - acc: 0.6200 - val_loss: 0.6466 - val_acc: 0.6289
Epoch 4/20
21900/21900 [==============================] - 139s 6ms/step - loss: 0.6453 - acc: 0.6294 - val_loss: 0.6302 - val_acc: 0.6463
Epoch 5/20
21900/21900 [==============================] - 139s 6ms/step - loss: 0.6084 - acc: 0.6682 - val_loss: 0.5222 - val_acc: 0.7372
Epoch 6/20
21900/21900 [==============================] - 138s 6ms/step - loss: 0.4660 - acc: 0.7715 - val_loss: 0.3917 - val_acc: 0.8103
Epoch 7/20
21900/21900 [==============================] - 138s 6ms/step - loss: 0.3468 - acc: 0.8391 - val_loss: 0.3657 - val_acc: 0.8210
Epoch 8/20
21900/21900 [==============================] - 137s 6ms/step - loss: 0.2891 - acc: 0.8673 - val_loss: 0.2555 - val_acc: 0.8805
Epoch 9/20
21900/21900 [==============================] - 138s 6ms/step - loss: 0.2646 - acc: 0.8807 - val_loss: 0.2868 - val_acc: 0.8719
Epoch 10/20
21900/21900 [==============================] - 138s 6ms/step - loss: 0.2456 - acc: 0.8888 - val_loss: 0.2651 - val_acc: 0.8774
Epoch 11/20
21900/21900 [==============================] - 137s 6ms/step - loss: 0.2315 - acc: 0.8945 - val_loss: 0.2153 - val_acc: 0.9018
Epoch 12/20
21900/21900 [==============================] - 139s 6ms/step - loss: 0.2190 - acc: 0.8993 - val_loss: 0.2069 - val_acc: 0.9060
Epoch 13/20
21900/21900 [==============================] - 138s 6ms/step - loss: 0.2134 - acc: 0.9030 - val_loss: 0.2369 - val_acc: 0.8970
Epoch 14/20
21900/21900 [==============================] - 139s 6ms/step - loss: 0.2131 - acc: 0.9028 - val_loss: 0.2010 - val_acc: 0.9075
Epoch 15/20
21900/21900 [==============================] - 139s 6ms/step - loss: 0.2050 - acc: 0.9063 - val_loss: 0.2037 - val_acc: 0.9107
Epoch 16/20
21900/21900 [==============================] - 138s 6ms/step - loss: 0.2026 - acc: 0.9075 - val_loss: 0.2163 - val_acc: 0.8958
Epoch 17/20
21900/21900 [==============================] - 138s 6ms/step - loss: 0.1999 - acc: 0.9084 - val_loss: 0.2127 - val_acc: 0.9014
Epoch 18/20
21900/21900 [==============================] - 140s 6ms/step - loss: 0.2052 - acc: 0.9067 - val_loss: 0.1960 - val_acc: 0.9090
Epoch 19/20
21900/21900 [==============================] - 139s 6ms/step - loss: 0.2071 - acc: 0.9058 - val_loss: 0.2024 - val_acc: 0.9077
Epoch 20/20
21900/21900 [==============================] - 138s 6ms/step - loss: 0.2057 - acc: 0.9066 - val_loss: 0.2069 - val_acc: 0.9071
sample  actual          predicted                       prediction error (over correct class)
0       1       [0.01940521 0.9805948 ]         0.01940518617630005
1       0       [0.98955137 0.01044867]         0.010448634624481201
2       0       [0.9666444  0.03335556]         0.03335559368133545
3       1       [0.0046918 0.9953082]   0.004691779613494873
4       0       [0.9979418  0.00205822]         0.0020582079887390137
5       1       [0.00216135 0.9978387 ]         0.0021613240242004395
6       0       [0.9962624  0.00373759]         0.00373762845993042
7       0       [0.9825215  0.01747859]         0.017478525638580322
8       0       [0.99329716 0.00670282]         0.006702840328216553
9       0       [0.9813153  0.01868465]         0.018684685230255127
10      1       [0.0088118  0.99118817]         0.0088118314743042
11      0       [0.9021423  0.09785765]         0.09785771369934082
12      1       [0.01120374 0.98879623]         0.011203765869140625
13      0       [0.9975817  0.00241828]         0.0024182796478271484
14      1       [0.00412103 0.99587893]         0.004121065139770508
15      0       [0.97532225 0.02467776]         0.024677753448486328
16      0       [0.95079494 0.04920506]         0.04920506477355957
17      1       [0.00288525 0.9971148 ]         0.0028852224349975586
18      0       [0.9850454  0.01495461]         0.014954626560211182
19      1       [0.00388824 0.9961118 ]         0.0038881897926330566
20      0       [0.9447887 0.0552113]   0.05521130561828613
21      1       [0.01526139 0.9847386 ]         0.015261411666870117
22      1       [0.00305634 0.99694365]         0.003056347370147705
23      1       [0.00283205 0.997168  ]         0.0028319954872131348
24      0       [0.9552949  0.04470508]         0.044705092906951904
25      0       [0.98447406 0.01552598]         0.0155259370803833
26      1       [0.94408906 0.05591097]         **0.9440890289843082**
27      1       [0.01257526 0.9874248 ]         0.012575209140777588
28      1       [0.00172795 0.99827206]         0.0017279386520385742
29      1       [0.0018659 0.9981341]   0.0018659234046936035
30      1       [0.01077967 0.9892203 ]         0.01077967882156372
31      0       [0.9964567  0.00354333]         0.0035433173179626465
32      1       [0.00579067 0.9942093 ]         0.00579071044921875
33      0       [0.9955318  0.00446817]         0.004468202590942383
34      1       [0.00184591 0.9981541 ]         0.0018458962440490723
35      0       [0.9828976  0.01710241]         0.017102420330047607
36      1       [0.00497089 0.99502915]         0.004970848560333252
37      0       [0.99732757 0.00267243]         0.002672433853149414
38      0       [0.99708563 0.00291433]         0.0029143691062927246
39      0       [0.9880266  0.01197331]         0.011973381042480469
40      0       [0.89959073 0.1004093 ]         0.10040926933288574
41      0       [0.9686018  0.03139819]         0.03139817714691162
42      0       [0.9055136  0.09448644]         0.09448641538619995
43      1       [0.9770179  0.02298217]         **0.9770178329199553**
44      0       [0.9075298  0.09247029]         0.09247022867202759
45      1       [0.00525542 0.99474454]         0.005255460739135742
46      1       [0.2694946  0.73050535]         0.2694946527481079
47      1       [0.00182275 0.9981773 ]         0.0018227100372314453
48      1       [0.00425295 0.995747  ]         0.004252970218658447
49      0       [0.97213167 0.02786827]         0.027868330478668213

================

Confusion matrix

================

        P       N
P       22      2
N       0       26
Precision: 1.0
Recall: 0.9166666666666666

TensorBoard event: 20180215_033530