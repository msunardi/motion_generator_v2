Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6844 - acc: 0.5660 - val_loss: 0.6838 - val_acc: 0.5607
Epoch 2/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6805 - acc: 0.5709 - val_loss: 0.6807 - val_acc: 0.5754
Epoch 3/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6640 - acc: 0.6017 - val_loss: 0.6773 - val_acc: 0.5989
Epoch 4/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6595 - acc: 0.6095 - val_loss: 0.6777 - val_acc: 0.5914
Epoch 5/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6561 - acc: 0.6176 - val_loss: 0.6667 - val_acc: 0.5972
Epoch 6/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6543 - acc: 0.6166 - val_loss: 0.6591 - val_acc: 0.6125
Epoch 7/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6516 - acc: 0.6241 - val_loss: 0.6630 - val_acc: 0.6155
Epoch 8/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6480 - acc: 0.6284 - val_loss: 0.6651 - val_acc: 0.6093
Epoch 9/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6471 - acc: 0.6293 - val_loss: 0.6494 - val_acc: 0.6213
Epoch 10/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6434 - acc: 0.6335 - val_loss: 0.6487 - val_acc: 0.6245
Epoch 11/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6425 - acc: 0.6355 - val_loss: 0.6437 - val_acc: 0.6308
Epoch 12/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6389 - acc: 0.6394 - val_loss: 0.6463 - val_acc: 0.6268
Epoch 13/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6353 - acc: 0.6430 - val_loss: 0.6354 - val_acc: 0.6310
Epoch 14/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.6269 - acc: 0.6503 - val_loss: 0.6270 - val_acc: 0.6454
Epoch 15/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.6181 - acc: 0.6591 - val_loss: 0.6060 - val_acc: 0.6712
Epoch 16/20
11450/11450 [==============================] - 27s 2ms/step - loss: 0.6063 - acc: 0.6682 - val_loss: 0.6207 - val_acc: 0.6501
Epoch 17/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.5826 - acc: 0.6888 - val_loss: 0.5788 - val_acc: 0.6780
Epoch 18/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.5465 - acc: 0.7172 - val_loss: 0.5600 - val_acc: 0.7001
Epoch 19/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.5102 - acc: 0.7443 - val_loss: 0.5295 - val_acc: 0.7235
Epoch 20/20
11450/11450 [==============================] - 27s 2ms/step - loss: 0.4627 - acc: 0.7721 - val_loss: 0.4737 - val_acc: 0.7546

model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_14 (GRU)                 (50, 50, 64)              12672     
_________________________________________________________________
gru_15 (GRU)                 (50, 50, 128)             74112     
_________________________________________________________________
dense_8 (Dense)              (50, 50, 2)               258       
=================================================================
Total params: 87,042
Trainable params: 87,042
Non-trainable params: 0
_________________________________________________________________

sample  actual          predicted                       prediction error (over correct class)
0       0       [0.8094949  0.19050507]         0.19050508737564087
1       0       [0.64158636 0.35841367]         0.3584136366844177
2       0       [0.6072006  0.39279932]         0.39279937744140625
3       0       [0.3884469  0.61155313]         **0.6115531027317047**
4       1       [0.05993918 0.9400608 ]         0.05993920564651489
5       1       [0.06126264 0.9387374 ]         0.06126260757446289
6       0       [0.4587718  0.54122823]         **0.5412282049655914**
7       1       [0.02679819 0.97320175]         0.026798248291015625
8       1       [0.04298208 0.95701784]         0.04298216104507446
9       0       [0.90557224 0.09442779]         0.09442776441574097
10      1       [0.05172285 0.9482772 ]         0.051722824573516846
11      1       [0.04114311 0.9588568 ]         0.041143178939819336
12      1       [0.11606224 0.8839377 ]         0.11606228351593018
13      0       [0.9638556  0.03614433]         0.036144375801086426
14      1       [0.04256549 0.9574345 ]         0.04256552457809448
15      0       [0.8650116  0.13498844]         0.1349884271621704
16      1       [0.01903419 0.98096573]         0.019034266471862793
17      1       [0.03134799 0.968652  ]         0.03134799003601074
18      1       [0.05391101 0.946089  ]         0.05391097068786621
19      0       [0.86276156 0.13723852]         0.13723844289779663
20      1       [0.03689834 0.9631017 ]         0.03689831495285034
21      1       [0.8112846  0.18871546]         **0.8112845420837402**
22      0       [0.93739444 0.06260559]         0.06260555982589722
23      0       [0.89351374 0.10648626]         0.1064862608909607
24      0       [0.53414094 0.46585906]         **0.465859055519104**
25      0       [0.2849763 0.7150237]   **0.7150236964225769**
26      1       [0.03189662 0.96810335]         0.03189665079116821
27      1       [0.03035459 0.9696454 ]         0.030354619026184082
28      0       [0.39618477 0.6038152 ]         **0.6038152277469635**
29      0       [0.55207187 0.44792816]         **0.44792813062667847**
30      1       [0.04149514 0.9585048 ]         0.04149520397186279
31      1       [0.7587494 0.2412505]   **0.7587494999170303**
32      1       [0.04646018 0.9535398 ]         0.04646021127700806
33      1       [0.6810408  0.31895915]         **0.6810408532619476**
34      1       [0.04734999 0.95265   ]         0.04734998941421509
35      1       [0.30392772 0.6960723 ]         0.3039277195930481
36      0       [0.59748757 0.40251246]         **0.40251243114471436**
37      1       [0.04585134 0.95414865]         0.04585134983062744
38      1       [0.01464204 0.985358  ]         0.014642000198364258
39      0       [0.48232734 0.51767266]         **0.5176726579666138**
40      1       [0.03541658 0.96458346]         0.03541654348373413
41      0       [0.61395913 0.38604084]         0.3860408663749695
42      1       [0.04757566 0.95242435]         0.04757565259933472
43      0       [0.94827527 0.05172478]         0.05172473192214966
44      0       [0.83134395 0.1686561 ]         0.16865605115890503
45      1       [0.02633879 0.9736612 ]         0.026338815689086914
46      1       [0.0664666  0.93353343]         0.0664665699005127
47      1       [0.02747777 0.9725222 ]         0.027477800846099854
48      0       [0.93694645 0.06305359]         0.06305354833602905
49      0       [0.8309765  0.16902356]         0.1690235137939453

================

Confusion matrix

================

        P       N
P       25      3
N       8       14

Logfile:
20180213_185619