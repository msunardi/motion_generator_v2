Layer (type)                 Output Shape              Param #   
=================================================================
gru_3 (GRU)                  (50, 50, 64)              12672     
_________________________________________________________________
gru_4 (GRU)                  (50, 50, 256)             246528    
_________________________________________________________________
dense_2 (Dense)              (50, 50, 2)               514       
=================================================================
Total params: 259,714
Trainable params: 259,714
Non-trainable params: 0
_________________________________________________________________
None
Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 51s 4ms/step - loss: 0.6851 - acc: 0.5649 - val_loss: 0.6802 - val_acc: 0.5664
Epoch 2/20
11450/11450 [==============================] - 51s 4ms/step - loss: 0.6815 - acc: 0.5668 - val_loss: 0.6722 - val_acc: 0.5978
Epoch 3/20
11450/11450 [==============================] - 51s 4ms/step - loss: 0.6656 - acc: 0.6006 - val_loss: 0.6596 - val_acc: 0.6023
Epoch 4/20
11450/11450 [==============================] - 51s 4ms/step - loss: 0.6602 - acc: 0.6116 - val_loss: 0.6563 - val_acc: 0.6189
Epoch 5/20
11450/11450 [==============================] - 51s 4ms/step - loss: 0.6560 - acc: 0.6182 - val_loss: 0.6414 - val_acc: 0.6385
Epoch 6/20
11450/11450 [==============================] - 51s 4ms/step - loss: 0.6536 - acc: 0.6213 - val_loss: 0.6385 - val_acc: 0.6418
Epoch 7/20
11450/11450 [==============================] - 51s 4ms/step - loss: 0.6481 - acc: 0.6252 - val_loss: 0.6487 - val_acc: 0.6309
Epoch 8/20
11450/11450 [==============================] - 52s 5ms/step - loss: 0.6447 - acc: 0.6320 - val_loss: 0.6434 - val_acc: 0.6384
Epoch 9/20
11450/11450 [==============================] - 54s 5ms/step - loss: 0.6346 - acc: 0.6426 - val_loss: 0.6202 - val_acc: 0.6550
Epoch 10/20
11450/11450 [==============================] - 56s 5ms/step - loss: 0.6239 - acc: 0.6510 - val_loss: 0.5983 - val_acc: 0.6754
Epoch 11/20
11450/11450 [==============================] - 65s 6ms/step - loss: 0.6030 - acc: 0.6698 - val_loss: 0.5777 - val_acc: 0.6916
Epoch 12/20
11450/11450 [==============================] - 62s 5ms/step - loss: 0.5679 - acc: 0.7015 - val_loss: 0.5403 - val_acc: 0.7261
Epoch 13/20
11450/11450 [==============================] - 54s 5ms/step - loss: 0.5274 - acc: 0.7303 - val_loss: 0.5029 - val_acc: 0.7518
Epoch 14/20
11450/11450 [==============================] - 54s 5ms/step - loss: 0.4831 - acc: 0.7610 - val_loss: 0.4588 - val_acc: 0.7772
Epoch 15/20
11450/11450 [==============================] - 54s 5ms/step - loss: 0.4429 - acc: 0.7857 - val_loss: 0.4140 - val_acc: 0.8041
Epoch 16/20
11450/11450 [==============================] - 53s 5ms/step - loss: 0.4060 - acc: 0.8063 - val_loss: 0.4027 - val_acc: 0.8126
Epoch 17/20
11450/11450 [==============================] - 54s 5ms/step - loss: 0.3814 - acc: 0.8189 - val_loss: 0.4091 - val_acc: 0.8172
Epoch 18/20
11450/11450 [==============================] - 54s 5ms/step - loss: 0.3632 - acc: 0.8271 - val_loss: 0.3223 - val_acc: 0.8507
Epoch 19/20
11450/11450 [==============================] - 60s 5ms/step - loss: 0.3458 - acc: 0.8388 - val_loss: 0.3015 - val_acc: 0.8620
Epoch 20/20
11450/11450 [==============================] - 60s 5ms/step - loss: 0.3295 - acc: 0.8466 - val_loss: 0.2895 - val_acc: 0.8681
sample  actual          predicted                       prediction error (over correct class)
0       1       [0.01649463 0.9835053 ]         0.016494691371917725
1       1       [0.01419781 0.98580223]         0.014197766780853271
2       1       [0.89189434 0.10810567]         **0.8918943330645561**
3       0       [0.96367466 0.03632526]         0.03632533550262451
4       0       [0.95133907 0.04866096]         0.04866093397140503
5       1       [0.00669137 0.99330866]         0.006691336631774902
6       1       [0.00837309 0.99162686]         0.008373141288757324
7       0       [0.989661 0.010339]     0.010339021682739258
8       0       [0.97216845 0.02783161]         0.027831554412841797
9       0       [0.92077374 0.07922627]         0.07922625541687012
10      1       [0.00948794 0.9905121 ]         0.009487926959991455
11      0       [0.9567669  0.04323308]         0.04323309659957886
12      1       [0.01044318 0.98955685]         0.010443150997161865
13      1       [0.01510511 0.98489493]         0.015105068683624268
14      1       [0.00784358 0.9921564 ]         0.007843613624572754
15      0       [0.98906904 0.01093092]         0.010930955410003662
16      0       [0.9755082  0.02449171]         0.02449178695678711
17      1       [0.01335096 0.9866491 ]         0.013350903987884521
18      0       [0.96828943 0.03171051]         0.03171056509017944
19      1       [0.00673925 0.9932607 ]         0.006739318370819092
20      1       [0.01040512 0.98959494]         0.01040506362915039
21      0       [0.9684516  0.03154839]         0.031548380851745605
22      0       [0.89810157 0.10189849]         0.1018984317779541
23      0       [0.9851577  0.01484232]         0.01484227180480957
24      0       [0.9562222  0.04377788]         0.04377782344818115
25      1       [0.01556514 0.9844349 ]         0.015565097332000732
26      1       [0.00542412 0.99457586]         0.005424141883850098
27      0       [0.98294294 0.01705706]         0.017057061195373535
28      0       [0.9899485  0.01005147]         0.010051488876342773
29      1       [0.00845892 0.9915411 ]         0.008458912372589111
30      0       [0.73433524 0.2656648 ]         0.2656647562980652
31      0       [0.9911602  0.00883984]         0.008839786052703857
32      0       [0.9403347  0.05966536]         0.05966532230377197
33      0       [0.94971937 0.05028065]         0.050280630588531494
34      1       [0.01095973 0.98904026]         0.010959744453430176
35      1       [0.00808278 0.9919172 ]         0.008082807064056396
36      1       [0.02714278 0.9728572 ]         0.027142822742462158
37      0       [0.9902221  0.00977792]         0.00977790355682373
38      1       [0.8587172  0.14128284]         **0.8587171584367752**
39      1       [0.15896045 0.84103954]         0.1589604616165161
40      1       [0.0045312  0.99546885]         0.004531145095825195
41      0       [0.49933335 0.5006666 ]         **0.5006666481494904**
42      0       [0.9769121  0.02308795]         0.023087918758392334
43      1       [0.00858433 0.9914157 ]         0.008584320545196533
44      0       [0.8940611  0.10593884]         0.10593891143798828
45      1       [0.00583896 0.99416107]         0.005838930606842041
46      0       [0.9887355  0.01126447]         0.011264503002166748
47      1       [0.00613955 0.9938605 ]         0.006139516830444336
48      1       [0.01074741 0.9892526 ]         0.010747373104095459
49      1       [0.00706028 0.9929397 ]         0.00706028938293457

================

Confusion matrix

================

        P       N
P       24      2
N       1       23
Precision: 0.96
Recall: 0.9230769230769231