model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_1 (GRU)                  (50, 50, 64)              12672     
_________________________________________________________________
gru_2 (GRU)                  (50, 50, 128)             74112     
_________________________________________________________________
dense_1 (Dense)              (50, 50, 2)               258       
=================================================================
Total params: 87,042
Trainable params: 87,042
Non-trainable params: 0
_________________________________________________________________


Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 36s 3ms/step - loss: 0.6844 - acc: 0.5642 - val_loss: 0.6810 - val_acc: 0.5700
Epoch 2/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.6788 - acc: 0.5737 - val_loss: 0.6616 - val_acc: 0.6106
Epoch 3/20
11450/11450 [==============================] - 36s 3ms/step - loss: 0.6644 - acc: 0.6011 - val_loss: 0.6645 - val_acc: 0.6075
Epoch 4/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.6610 - acc: 0.6075 - val_loss: 0.6528 - val_acc: 0.6192
Epoch 5/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.6585 - acc: 0.6119 - val_loss: 0.6500 - val_acc: 0.6331
Epoch 6/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.6552 - acc: 0.6169 - val_loss: 0.6553 - val_acc: 0.6163
Epoch 7/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.6544 - acc: 0.6183 - val_loss: 0.6461 - val_acc: 0.6311
Epoch 8/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.6510 - acc: 0.6224 - val_loss: 0.6496 - val_acc: 0.6291
Epoch 9/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.6487 - acc: 0.6254 - val_loss: 0.6386 - val_acc: 0.6351
Epoch 10/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.6476 - acc: 0.6294 - val_loss: 0.6323 - val_acc: 0.6418
Epoch 11/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.6445 - acc: 0.6302 - val_loss: 0.6312 - val_acc: 0.6418
Epoch 12/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.6399 - acc: 0.6383 - val_loss: 0.6276 - val_acc: 0.6491
Epoch 13/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.6349 - acc: 0.6417 - val_loss: 0.6350 - val_acc: 0.6464
Epoch 14/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.6290 - acc: 0.6470 - val_loss: 0.6167 - val_acc: 0.6618
Epoch 15/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.6201 - acc: 0.6560 - val_loss: 0.5961 - val_acc: 0.6817
Epoch 16/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.5992 - acc: 0.6754 - val_loss: 0.5771 - val_acc: 0.6983
Epoch 17/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.5598 - acc: 0.7089 - val_loss: 0.5155 - val_acc: 0.7500
Epoch 18/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.5108 - acc: 0.7418 - val_loss: 0.4549 - val_acc: 0.7817
Epoch 19/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.4658 - acc: 0.7703 - val_loss: 0.4431 - val_acc: 0.7893
Epoch 20/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.4338 - acc: 0.7884 - val_loss: 0.3767 - val_acc: 0.8256

# TEST1
sample  actual          predicted                       prediction error (over correct class)
0       1       [0.04552606 0.9544739 ]         0.045526087284088135
1       0       [0.9584859  0.04151407]         0.04151409864425659
2       1       [0.01520301 0.984797  ]         0.015202999114990234
3       0       [0.9684812 0.0315189]   0.03151881694793701
4       1       [0.02240681 0.9775932 ]         0.022406816482543945
5       0       [0.9345796  0.06542043]         0.06542038917541504
6       0       [0.79833156 0.2016684 ]         0.20166844129562378
7       1       [0.2546435 0.7453565]   0.2546434998512268
8       1       [0.02276703 0.977233  ]         0.02276700735092163
9       0       [0.9813005  0.01869952]         0.0186995267868042
10      0       [0.9591274  0.04087263]         0.04087257385253906
11      0       [0.48036334 0.5196367 ]         **0.5196366608142853**
12      1       [0.07647917 0.92352086]         0.07647913694381714
13      0       [0.8664082  0.13359176]         0.13359177112579346
14      0       [0.7168256 0.2831744]   0.28317439556121826
15      0       [0.7992742  0.20072581]         0.20072579383850098
16      1       [0.03242809 0.96757185]         0.03242814540863037
17      0       [0.6309412  0.36905885]         0.3690587878227234
18      0       [0.7897998  0.21020019]         0.21020019054412842
19      1       [0.04867317 0.95132685]         0.048673152923583984
20      1       [0.02745971 0.97254026]         0.02745974063873291
21      0       [0.7050089  0.29499105]         0.29499107599258423
22      1       [0.03717128 0.96282876]         0.037171244621276855
23      0       [0.9185555  0.08144447]         0.08144450187683105
24      1       [0.04318397 0.956816  ]         0.043183982372283936
25      1       [0.0186699 0.9813301]   0.01866990327835083
26      0       [0.97267014 0.02732986]         0.027329862117767334
27      1       [0.06902586 0.9309742 ]         0.06902581453323364
28      1       [0.04956995 0.9504301 ]         0.049569904804229736
29      0       [0.73011917 0.26988077]         0.26988083124160767
30      0       [0.97249323 0.02750677]         0.027506768703460693
31      1       [0.03192125 0.9680788 ]         0.031921207904815674
32      0       [0.91950667 0.08049329]         0.08049333095550537
33      1       [0.7186109  0.28138912]         **0.7186108827590942**
34      0       [0.9484629  0.05153708]         0.05153709650039673
35      0       [0.7212319  0.27876815]         0.2787681221961975
36      0       [0.85500747 0.14499259]         0.14499253034591675
37      1       [0.7840405  0.21595947]         **0.7840405255556107**
38      0       [0.8973139  0.10268616]         0.10268610715866089
39      1       [0.05304353 0.94695646]         0.05304354429244995
40      0       [0.94921964 0.05078039]         0.05078035593032837
41      0       [0.91209173 0.08790824]         0.08790826797485352
42      0       [0.95497614 0.0450239 ]         0.04502385854721069
43      0       [0.9674522  0.03254775]         0.03254777193069458
44      1       [0.03460336 0.9653967 ]         0.0346032977104187
45      1       [0.02331    0.97669005]         0.023309946060180664
46      0       [0.79036814 0.20963186]         0.20963186025619507
47      1       [0.02875147 0.97124857]         0.0287514328956604
48      1       [0.01708151 0.9829185 ]         0.017081499099731445
49      1       [0.11263113 0.8873689 ]         0.11263108253479004

================

Confusion matrix

================

        P       N
P       21      2
N       1       26
Precision: 0.9545454545454546
Recall: 0.9130434782608695

# TEST2
sample  actual          predicted                       prediction error (over correct class)
0       0       [0.62480795 0.375192  ]         0.3751920461654663
1       1       [0.03932624 0.96067375]         0.039326250553131104
2       0       [0.5611314 0.4388686]   **0.43886858224868774**
3       0       [0.97164613 0.0283538 ]         0.028353869915008545
4       1       [0.11923417 0.88076586]         0.11923414468765259
5       1       [0.02785935 0.9721407 ]         0.027859270572662354
6       0       [0.88145417 0.11854584]         0.11854583024978638
7       1       [0.02688343 0.9731165 ]         0.026883482933044434
8       1       [0.0237692 0.9762308]   0.02376919984817505
9       1       [0.23960173 0.76039827]         0.239601731300354
10      1       [0.09627761 0.9037224 ]         0.09627759456634521
11      0       [0.90489703 0.09510294]         0.09510296583175659
12      0       [0.98670876 0.01329125]         0.013291239738464355
13      0       [0.8982681  0.10173187]         0.10173189640045166
14      0       [0.9277098  0.07229017]         0.07229018211364746
15      0       [0.9346375  0.06536252]         0.06536251306533813
16      0       [0.7743108 0.2256892]   0.22568917274475098
17      0       [0.77844316 0.22155684]         0.22155684232711792
18      0       [0.5416299 0.4583701]   **0.4583700895309448**
19      0       [0.9876231  0.01237693]         0.012376904487609863
20      0       [0.89072526 0.10927482]         0.10927474498748779
21      0       [0.955359   0.04464097]         0.04464101791381836
22      0       [0.9166433  0.08335674]         0.08335667848587036
23      0       [0.80186033 0.19813971]         0.19813966751098633
24      0       [0.38919178 0.6108082 ]         **0.6108082234859467**
25      1       [0.04486349 0.95513654]         0.04486346244812012
26      1       [0.04573555 0.95426446]         0.04573553800582886
27      0       [0.9867597  0.01324036]         0.013240277767181396
28      0       [0.9786149  0.02138501]         0.0213850736618042
29      0       [0.74345744 0.25654256]         0.2565425634384155
30      0       [0.91494626 0.08505375]         0.08505374193191528
31      0       [0.87734014 0.12265988]         0.12265986204147339
32      0       [0.7204612  0.27953878]         0.2795388102531433
33      0       [0.9308478  0.06915212]         0.06915217638015747
34      1       [0.01759262 0.98240745]         0.017592549324035645
35      0       [0.96108013 0.03891987]         0.03891986608505249
36      0       [0.9682349  0.03176513]         0.031765103340148926
37      0       [0.7831636  0.21683641]         0.21683639287948608
38      0       [0.9657567  0.03424327]         0.03424328565597534
39      0       [0.97306585 0.02693412]         0.026934146881103516
40      0       [0.8528213  0.14717868]         0.14717870950698853
41      0       [0.8462328  0.15376721]         0.15376722812652588
42      1       [0.03278734 0.9672126 ]         0.03278738260269165
43      0       [0.9206123  0.07938774]         0.07938772439956665
44      1       [0.01943477 0.98056525]         0.019434750080108643
45      1       [0.03251133 0.9674887 ]         0.03251129388809204
46      1       [0.01573559 0.98426443]         0.01573556661605835
47      0       [0.84646785 0.15353212]         0.15353214740753174
48      0       [0.93746954 0.06253047]         0.06253045797348022
49      1       [0.03505456 0.9649454 ]         0.03505462408065796

================

Confusion matrix

================

        P       N
P       15      0
N       3       32
Precision: 0.8333333333333334
Recall: 1.0

In [10]: 