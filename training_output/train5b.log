3rd retrain

Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2497 - acc: 0.8836 - val_loss: 0.2472 - val_acc: 0.8850
Epoch 2/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2507 - acc: 0.8832 - val_loss: 0.2367 - val_acc: 0.8878
Epoch 3/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.2453 - acc: 0.8856 - val_loss: 0.2473 - val_acc: 0.8831
Epoch 4/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2432 - acc: 0.8866 - val_loss: 0.2363 - val_acc: 0.8894
Epoch 5/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2421 - acc: 0.8869 - val_loss: 0.2433 - val_acc: 0.8874
Epoch 6/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2418 - acc: 0.8877 - val_loss: 0.2362 - val_acc: 0.8915
Epoch 7/20
11450/11450 [==============================] - 28s 2ms/step - loss: 0.2350 - acc: 0.8898 - val_loss: 0.2354 - val_acc: 0.8913
Epoch 8/20
11450/11450 [==============================] - 27s 2ms/step - loss: 0.2362 - acc: 0.8898 - val_loss: 0.2407 - val_acc: 0.8832
Epoch 9/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.2360 - acc: 0.8899 - val_loss: 0.2384 - val_acc: 0.8868
Epoch 10/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.2330 - acc: 0.8914 - val_loss: 0.2291 - val_acc: 0.8927
Epoch 11/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.2341 - acc: 0.8911 - val_loss: 0.2298 - val_acc: 0.8916
Epoch 12/20
11450/11450 [==============================] - 44s 4ms/step - loss: 0.2316 - acc: 0.8912 - val_loss: 0.2521 - val_acc: 0.8866
Epoch 13/20
11450/11450 [==============================] - 34s 3ms/step - loss: 0.2340 - acc: 0.8915 - val_loss: 0.2936 - val_acc: 0.8552
Epoch 14/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.2314 - acc: 0.8920 - val_loss: 0.2427 - val_acc: 0.8894
Epoch 15/20
11450/11450 [==============================] - 29s 3ms/step - loss: 0.2326 - acc: 0.8917 - val_loss: 0.2282 - val_acc: 0.8873
Epoch 16/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.2288 - acc: 0.8938 - val_loss: 0.2178 - val_acc: 0.8960
Epoch 17/20
11450/11450 [==============================] - 29s 3ms/step - loss: 0.2256 - acc: 0.8943 - val_loss: 0.2307 - val_acc: 0.8854
Epoch 18/20
11450/11450 [==============================] - 29s 3ms/step - loss: 0.2309 - acc: 0.8926 - val_loss: 0.2240 - val_acc: 0.8951
Epoch 19/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.2235 - acc: 0.8956 - val_loss: 0.2231 - val_acc: 0.8933
Epoch 20/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.2255 - acc: 0.8948 - val_loss: 0.2251 - val_acc: 0.8946


model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_14 (GRU)                 (50, 50, 64)              12672     
_________________________________________________________________
gru_15 (GRU)                 (50, 50, 128)             74112     
_________________________________________________________________
dense_8 (Dense)              (50, 50, 2)               258       
=================================================================
Total params: 87,042
Trainable params: 87,042
Non-trainable params: 0
_________________________________________________________________

sample  actual          predicted                       prediction error (over correct class)
0       0       [0.99064094 0.00935903]         0.00935906171798706
1       0       [0.84341305 0.15658692]         0.15658694505691528
2       0       [0.84643334 0.15356661]         0.1535666584968567
3       0       [0.9923867 0.0076133]   0.0076133012771606445
4       1       [0.00261128 0.9973888 ]         0.002611219882965088
5       1       [0.01474044 0.98525953]         0.014740467071533203
6       0       [0.9956899  0.00431012]         0.0043100714683532715
7       1       [0.00578926 0.9942107 ]         0.005789279937744141
8       1       [0.00257471 0.9974253 ]         0.0025746822357177734
9       0       [0.9906102  0.00938982]         0.009389817714691162
10      1       [0.00445516 0.9955448 ]         0.004455208778381348
11      1       [0.00472976 0.9952702 ]         0.004729807376861572
12      1       [0.02047235 0.97952765]         0.020472347736358643
13      0       [0.9972103  0.00278967]         0.0027896761894226074
14      1       [0.00244493 0.9975551 ]         0.002444922924041748
15      0       [0.99874914 0.00125088]         0.0012508630752563477
16      1       [0.00631154 0.9936885 ]         0.006311476230621338
17      1       [0.00782934 0.99217063]         0.007829368114471436
18      1       [0.00703129 0.99296874]         0.007031261920928955
19      0       [0.99768555 0.00231442]         0.002314448356628418
20      1       [0.01192541 0.9880746 ]         0.01192539930343628
21      1       [0.0073421  0.99265784]         0.007342159748077393
22      0       [0.99506396 0.00493606]         0.004936039447784424
23      0       [0.9941988  0.00580117]         0.005801200866699219
24      0       [0.6696257  0.33037433]         0.3303743004798889
25      0       [0.9915976  0.00840248]         0.008402407169342041
26      1       [0.003408   0.99659204]         0.0034079551696777344
27      1       [0.0152739  0.98472613]         0.015273869037628174
28      0       [0.99508095 0.00491901]         0.0049190521240234375
29      0       [0.69703805 0.30296198]         0.30296194553375244
30      1       [0.01553774 0.98446226]         0.015537738800048828
31      1       [0.0044134  0.99558663]         0.0044133663177490234
32      1       [0.36940998 0.63059   ]         0.36940997838974
33      1       [0.62637144 0.37362853]         **0.6263714730739594**
34      1       [0.00214185 0.9978581 ]         0.002141892910003662
35      1       [0.71150774 0.2884923 ]         **0.7115077078342438**
36      0       [0.48219973 0.5178003 ]         **0.5178002715110779**
37      1       [0.02295097 0.97704905]         0.022950947284698486
38      1       [0.00442674 0.9955733 ]         0.004426717758178711
39      0       [0.991706   0.00829402]         0.008293986320495605
40      1       [0.0093009 0.9906991]   0.00930088758468628
41      0       [0.9951363  0.00486371]         0.0048636794090271
42      1       [0.00454177 0.99545825]         0.004541754722595215
43      0       [0.9924386  0.00756139]         0.007561385631561279
44      0       [0.99119616 0.00880379]         0.008803844451904297
45      1       [0.0025778 0.9974222]   0.0025777816772460938
46      1       [0.02195893 0.97804105]         0.02195894718170166
47      1       [0.00766065 0.9923394 ]         0.007660627365112305
48      0       [0.98567826 0.01432179]         0.014321744441986084
49      0       [0.9912943 0.0087057]   0.008705675601959229

================

Confusion matrix

================

        P       N
P       26      2
N       1       21
Precision: 0.9629629629629629
Recall: 0.9285714285714286

# Shuffled test data:
sample  actual          predicted                       prediction error (over correct class)
0       0       [0.9570599  0.04294015]         0.04294008016586304
1       0       [0.9984816  0.00151849]         0.0015184283256530762
2       0       [0.99396896 0.00603099]         0.006031036376953125
3       0       [0.989618   0.01038198]         0.010381996631622314
4       1       [0.01552988 0.9844701 ]         0.015529870986938477
5       0       [0.9706981  0.02930184]         0.029301881790161133
6       0       [0.9834856  0.01651435]         0.01651442050933838
7       0       [0.99025774 0.00974231]         0.009742259979248047
8       1       [0.00997176 0.9900282 ]         0.009971797466278076
9       1       [0.00349382 0.9965062 ]         0.003493785858154297
10      0       [0.9939725  0.00602749]         0.006027519702911377
11      1       [0.00205952 0.9979405 ]         0.0020595192909240723
12      0       [0.5118372  0.48816282]         **0.4881628155708313**
13      0       [0.989684   0.01031595]         0.010316014289855957
14      0       [0.99393034 0.00606968]         0.006069660186767578
15      1       [0.00403743 0.99596256]         0.004037439823150635
16      1       [0.04918495 0.9508151 ]         0.04918491840362549
17      1       [0.00225248 0.9977475 ]         0.002252519130706787
18      1       [0.01447809 0.9855219 ]         0.014478087425231934
19      1       [0.01833958 0.98166037]         0.01833963394165039
20      1       [0.00193168 0.9980683 ]         0.0019317269325256348
21      0       [0.9739821 0.0260179]   0.02601790428161621
22      1       [0.01341003 0.98659   ]         0.013409972190856934
23      0       [0.9881246  0.01187539]         0.011875391006469727
24      0       [0.990377   0.00962297]         0.00962299108505249
25      1       [0.01005102 0.989949  ]         0.01005101203918457
26      0       [0.99023825 0.00976175]         0.0097617506980896
27      1       [0.01031387 0.9896861 ]         0.010313928127288818
28      1       [0.01558949 0.9844105 ]         0.015589475631713867
29      1       [0.00539278 0.99460727]         0.005392730236053467
30      1       [0.01048461 0.9895154 ]         0.010484576225280762
31      1       [0.00427521 0.9957248 ]         0.004275202751159668
32      1       [0.00162761 0.9983724 ]         0.0016276240348815918
33      1       [0.00393169 0.99606824]         0.003931760787963867
34      1       [0.02151517 0.9784848 ]         0.021515190601348877
35      1       [0.01032622 0.98967385]         0.010326147079467773
36      1       [0.02680405 0.9731959 ]         0.026804089546203613
37      0       [0.9660019  0.03399811]         0.033998072147369385
38      1       [0.00783658 0.9921634 ]         0.007836580276489258
39      0       [0.99276674 0.00723328]         0.007233262062072754
40      1       [0.00982107 0.99017894]         0.009821057319641113
41      0       [0.9976548  0.00234512]         0.0023452043533325195
42      1       [0.2726302 0.7273698]   0.2726302146911621
43      0       [0.9864885  0.01351152]         0.013511478900909424
44      1       [0.00492746 0.99507254]         0.004927456378936768
45      1       [0.00525614 0.9947438 ]         0.005256175994873047
46      1       [0.007095  0.9929051]   0.007094919681549072
47      0       [0.99073726 0.0092628 ]         0.00926274061203003
48      1       [0.00494388 0.99505615]         0.00494384765625
49      1       [0.09246154 0.90753853]         0.0924614667892456

================

Confusion matrix

================

        P       N
P       30      0
N       1       19
Precision: 0.967741935483871
Recall: 1.0