
model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_1 (GRU)                  (50, 50, 64)              12672     
_________________________________________________________________
gru_2 (GRU)                  (50, 50, 128)             74112     
_________________________________________________________________
dense_1 (Dense)              (50, 50, 2)               258       
=================================================================
Total params: 87,042
Trainable params: 87,042
Non-trainable params: 0
_________________________________________________________________

Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.4059 - acc: 0.8043 - val_loss: 0.3981 - val_acc: 0.8219
Epoch 2/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.3792 - acc: 0.8184 - val_loss: 0.3488 - val_acc: 0.8392
Epoch 3/20
11450/11450 [==============================] - 30s 3ms/step - loss: 0.3627 - acc: 0.8260 - val_loss: 0.3572 - val_acc: 0.8327
Epoch 4/20
11450/11450 [==============================] - 31s 3ms/step - loss: 0.3454 - acc: 0.8360 - val_loss: 0.4072 - val_acc: 0.8056
Epoch 5/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.3270 - acc: 0.8459 - val_loss: 0.3044 - val_acc: 0.8588
Epoch 6/20
11450/11450 [==============================] - 36s 3ms/step - loss: 0.3216 - acc: 0.8480 - val_loss: 0.3039 - val_acc: 0.8596
Epoch 7/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.3056 - acc: 0.8558 - val_loss: 0.3083 - val_acc: 0.8601
Epoch 8/20
11450/11450 [==============================] - 36s 3ms/step - loss: 0.3008 - acc: 0.8573 - val_loss: 0.2842 - val_acc: 0.8710
Epoch 9/20
11450/11450 [==============================] - 39s 3ms/step - loss: 0.2961 - acc: 0.8603 - val_loss: 0.2894 - val_acc: 0.8656
Epoch 10/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.2839 - acc: 0.8653 - val_loss: 0.2743 - val_acc: 0.8722
Epoch 11/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2800 - acc: 0.8694 - val_loss: 0.2646 - val_acc: 0.8795
Epoch 12/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2757 - acc: 0.8712 - val_loss: 0.2587 - val_acc: 0.8835
Epoch 13/20
11450/11450 [==============================] - 32s 3ms/step - loss: 0.2766 - acc: 0.8705 - val_loss: 0.2673 - val_acc: 0.8773
Epoch 14/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.2702 - acc: 0.8735 - val_loss: 0.2587 - val_acc: 0.8846
Epoch 15/20
11450/11450 [==============================] - 34s 3ms/step - loss: 0.2729 - acc: 0.8723 - val_loss: 0.2673 - val_acc: 0.8778
Epoch 16/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.2686 - acc: 0.8750 - val_loss: 0.2574 - val_acc: 0.8816
Epoch 17/20
11450/11450 [==============================] - 33s 3ms/step - loss: 0.2606 - acc: 0.8783 - val_loss: 0.3722 - val_acc: 0.8262
Epoch 18/20
11450/11450 [==============================] - 34s 3ms/step - loss: 0.2658 - acc: 0.8754 - val_loss: 0.2581 - val_acc: 0.8838
Epoch 19/20
11450/11450 [==============================] - 34s 3ms/step - loss: 0.2566 - acc: 0.8796 - val_loss: 0.2604 - val_acc: 0.8824
Epoch 20/20
11450/11450 [==============================] - 34s 3ms/step - loss: 0.2548 - acc: 0.8810 - val_loss: 0.2457 - val_acc: 0.8892

sample  actual          predicted                       prediction error (over correct class)
0       1       [0.02017886 0.9798211 ]         0.020178914070129395
1       1       [0.5758462  0.42415377]         **0.5758462250232697**
2       1       [0.00628691 0.9937131 ]         0.006286919116973877
3       0       [0.98848814 0.01151186]         0.01151186227798462
4       1       [0.31514496 0.684855  ]         0.31514501571655273
5       1       [0.18546903 0.8145309 ]         0.18546909093856812
6       1       [0.00936048 0.99063957]         0.009360432624816895
7       1       [0.01022491 0.98977506]         0.01022493839263916
8       1       [0.01133413 0.9886658 ]         0.01133418083190918
9       1       [0.71406335 0.28593668]         **0.7140633165836334**
10      0       [0.95003104 0.04996896]         0.04996895790100098
11      0       [0.99459076 0.0054093 ]         0.00540924072265625
12      0       [0.9657675  0.03423253]         0.034232497215270996
13      1       [0.00715067 0.99284935]         0.0071506500244140625
14      0       [0.60093254 0.3990675 ]         0.3990674614906311
15      1       [0.00789078 0.99210924]         0.007890760898590088
16      0       [0.99467385 0.00532612]         0.0053261518478393555
17      1       [0.03462719 0.9653728 ]         0.03462719917297363
18      0       [0.9949019  0.00509804]         0.005098104476928711
19      1       [0.01111162 0.98888844]         0.011111557483673096
20      1       [0.00727777 0.9927222 ]         0.007277786731719971
21      0       [0.80953115 0.19046883]         0.19046884775161743
22      1       [0.00548971 0.9945103 ]         0.005489706993103027
23      0       [0.9967192  0.00328084]         0.003280818462371826
24      0       [0.8929829  0.10701706]         0.10701709985733032
25      1       [0.0137392  0.98626083]         0.013739168643951416
26      0       [0.66496843 0.33503157]         0.33503156900405884
27      1       [0.0049623  0.99503773]         0.004962265491485596
28      0       [0.9954809  0.00451904]         0.004519104957580566
29      1       [0.01549687 0.98450315]         0.01549685001373291
30      1       [0.00378707 0.99621296]         0.0037870407104492188
31      0       [0.9933977  0.00660228]         0.006602287292480469
32      1       [0.6779492  0.32205078]         **0.6779492199420929**
33      0       [0.9921429  0.00785704]         0.007857084274291992
34      0       [0.9705509  0.02944913]         0.029449105262756348
35      1       [0.0025761 0.9974239]   0.002576112747192383
36      1       [0.00787772 0.99212223]         0.007877767086029053
37      1       [0.00966296 0.990337  ]         0.009662985801696777
38      0       [0.9917469  0.00825314]         0.008253097534179688
39      1       [0.00606115 0.9939388 ]         0.006061196327209473
40      1       [0.01342494 0.98657507]         0.013424932956695557
41      1       [0.00672206 0.99327797]         0.0067220330238342285
42      1       [0.00863348 0.99136645]         0.008633553981781006
43      0       [0.994079   0.00592104]         0.005921006202697754
44      1       [0.00409056 0.99590945]         0.004090547561645508
45      1       [0.00291759 0.9970824 ]         0.0029175877571105957
46      0       [0.99656445 0.00343554]         0.0034355521202087402
47      1       [0.00669625 0.9933037 ]         0.00669628381729126
48      1       [0.00458277 0.99541724]         0.004582762718200684
49      0       [0.9772679  0.02273202]         0.022732079029083252

================

Confusion matrix

================

        P       N
P       28      3
N       0       19
Precision: 1.0
Recall: 0.9032258064516129

# TEST2
sample  actual          predicted                       prediction error (over correct class)
0       0       [0.98370486 0.01629518]         0.016295135021209717
1       0       [0.9898816  0.01011842]         0.010118424892425537
2       0       [0.93703604 0.06296397]         0.06296396255493164
3       1       [0.00257591 0.9974241 ]         0.0025758743286132812
4       0       [0.9318615  0.06813852]         0.0681384801864624
5       1       [0.17094293 0.82905704]         0.17094296216964722
6       0       [0.9894772  0.01052278]         0.010522782802581787
7       1       [0.00658781 0.9934122 ]         0.006587803363800049
8       1       [0.00510217 0.9948978 ]         0.005102217197418213
9       1       [0.0505676 0.9494324]   0.050567626953125
10      0       [0.98383623 0.0161638 ]         0.016163766384124756
11      1       [0.5723737  0.42762634]         **0.5723736584186554**
12      0       [0.99596965 0.00403038]         0.004030346870422363
13      1       [0.20934069 0.79065937]         0.2093406319618225
14      0       [0.9677135  0.03228648]         0.03228652477264404
15      1       [0.00642071 0.99357927]         0.006420731544494629
16      0       [0.8905268  0.10947327]         0.10947322845458984
17      1       [0.00689409 0.99310595]         0.006894052028656006
18      1       [0.02318079 0.9768192 ]         0.023180782794952393
19      0       [0.8036683  0.19633171]         0.1963316798210144
20      0       [0.9958346  0.00416535]         0.0041654109954833984
21      0       [0.99704903 0.00295097]         0.0029509663581848145
22      0       [0.90776104 0.09223893]         0.09223896265029907
23      1       [0.00307204 0.9969279 ]         0.003072082996368408
24      1       [0.01224355 0.9877565 ]         0.012243509292602539
25      1       [0.02469693 0.97530305]         0.024696946144104004
26      0       [0.9916426  0.00835745]         0.008357405662536621
27      0       [0.7786516  0.22134845]         0.22134840488433838
28      0       [0.9484021  0.05159785]         0.05159789323806763
29      0       [0.9789257  0.02107437]         0.021074295043945312
30      0       [0.9928697  0.00713027]         0.007130324840545654
31      1       [0.00644471 0.99355525]         0.006444752216339111
32      0       [0.8182025  0.18179753]         0.18179750442504883
33      0       [0.9454223  0.05457769]         0.05457770824432373
34      1       [0.00263637 0.9973636 ]         0.0026363730430603027
35      0       [0.9954904  0.00450959]         0.004509627819061279
36      0       [0.925154   0.07484598]         0.07484602928161621
37      1       [0.00401717 0.9959829 ]         0.0040171146392822266
38      1       [0.10765531 0.8923447 ]         0.10765528678894043
39      0       [0.9939422  0.00605773]         0.006057798862457275
40      0       [0.99544734 0.0045526 ]         0.004552662372589111
41      0       [0.9960263  0.00397369]         0.003973722457885742
42      0       [0.9966702  0.00332984]         0.0033298134803771973
43      0       [0.9413613  0.05863865]         0.058638691902160645
44      1       [0.00247241 0.99752766]         0.0024723410606384277
45      1       [0.00574484 0.9942551 ]         0.005744874477386475
46      0       [0.9787394  0.02126055]         0.021260619163513184
47      1       [0.00484531 0.99515474]         0.004845261573791504
48      1       [0.04245381 0.9575462 ]         0.0424538254737854
49      1       [0.03211529 0.96788466]         0.03211534023284912

================

Confusion matrix

================

        P       N
P       21      1
N       0       28
Precision: 1.0
Recall: 0.9545454545454546
