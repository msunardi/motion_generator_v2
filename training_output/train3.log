Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 9s 775us/step - loss: 0.6837 - acc: 0.5668 - val_loss: 0.6851 - val_acc: 0.5564
Epoch 2/20
11450/11450 [==============================] - 9s 796us/step - loss: 0.6824 - acc: 0.5675 - val_loss: 0.6833 - val_acc: 0.5564
Epoch 3/20
11450/11450 [==============================] - 9s 786us/step - loss: 0.6816 - acc: 0.5675 - val_loss: 0.6814 - val_acc: 0.5564
Epoch 4/20
11450/11450 [==============================] - 10s 866us/step - loss: 0.6697 - acc: 0.5909 - val_loss: 0.6724 - val_acc: 0.5929
Epoch 5/20
11450/11450 [==============================] - 13s 1ms/step - loss: 0.6620 - acc: 0.6051 - val_loss: 0.6700 - val_acc: 0.5909
Epoch 6/20
11450/11450 [==============================] - 10s 871us/step - loss: 0.6570 - acc: 0.6139 - val_loss: 0.6692 - val_acc: 0.5971
Epoch 7/20
11450/11450 [==============================] - 9s 811us/step - loss: 0.6574 - acc: 0.6127 - val_loss: 0.6656 - val_acc: 0.5964
Epoch 8/20
11450/11450 [==============================] - 12s 1ms/step - loss: 0.6554 - acc: 0.6175 - val_loss: 0.6750 - val_acc: 0.6023
Epoch 9/20
11450/11450 [==============================] - 13s 1ms/step - loss: 0.6567 - acc: 0.6166 - val_loss: 0.6636 - val_acc: 0.6033
Epoch 10/20
11450/11450 [==============================] - 10s 902us/step - loss: 0.6544 - acc: 0.6193 - val_loss: 0.6676 - val_acc: 0.6041
Epoch 11/20
11450/11450 [==============================] - 9s 758us/step - loss: 0.6545 - acc: 0.6183 - val_loss: 0.6694 - val_acc: 0.6082
Epoch 12/20
11450/11450 [==============================] - 9s 765us/step - loss: 0.6547 - acc: 0.6183 - val_loss: 0.6648 - val_acc: 0.5987
Epoch 13/20
11450/11450 [==============================] - 9s 829us/step - loss: 0.6530 - acc: 0.6229 - val_loss: 0.6678 - val_acc: 0.6044
Epoch 14/20
11450/11450 [==============================] - 9s 745us/step - loss: 0.6526 - acc: 0.6221 - val_loss: 0.6608 - val_acc: 0.6084
Epoch 15/20
11450/11450 [==============================] - 9s 810us/step - loss: 0.6515 - acc: 0.6219 - val_loss: 0.6659 - val_acc: 0.6036
Epoch 16/20
11450/11450 [==============================] - 8s 721us/step - loss: 0.6513 - acc: 0.6230 - val_loss: 0.6618 - val_acc: 0.6055
Epoch 17/20
11450/11450 [==============================] - 9s 778us/step - loss: 0.6513 - acc: 0.6245 - val_loss: 0.6645 - val_acc: 0.6024
Epoch 18/20
11450/11450 [==============================] - 8s 728us/step - loss: 0.6501 - acc: 0.6253 - val_loss: 0.6612 - val_acc: 0.6110
Epoch 19/20
11450/11450 [==============================] - 8s 733us/step - loss: 0.6497 - acc: 0.6237 - val_loss: 0.6640 - val_acc: 0.6082
Epoch 20/20
11450/11450 [==============================] - 9s 746us/step - loss: 0.6490 - acc: 0.6262 - val_loss: 0.6633 - val_acc: 0.6027
Out[175]: <keras.callbacks.History at 0x7f18b2ec68d0>

model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_13 (GRU)                 (50, 50, 64)              12672     
_________________________________________________________________
dense_7 (Dense)              (50, 50, 2)               130       
=================================================================
Total params: 12,802
Trainable params: 12,802
Non-trainable params: 0
_________________________________________________________________

# TEST

sample  actual          predicted                       prediction error (over correct class)
0       1       [0.30329713 0.69670284]         0.30329716205596924
1       1       [0.3085535 0.6914465]   0.3085535168647766
2       0       [0.35118416 0.6488159 ]         **0.6488158404827118**
3       1       [0.3073745 0.6926255]   0.3073744773864746
4       1       [0.35646203 0.64353794]         0.3564620614051819
5       1       [0.72233    0.27767003]         **0.7223299741744995**
6       1       [0.30860883 0.6913912 ]         0.3086087703704834
7       1       [0.3320496  0.66795033]         0.3320496678352356
8       1       [0.08671704 0.91328293]         0.08671706914901733
9       1       [0.620593   0.37940693]         **0.6205930709838867**
10      0       [0.50590277 0.49409723]         **0.4940972328186035**
11      0       [0.4010528 0.5989472]   **0.5989471971988678**
12      1       [0.5304624  0.46953762]         **0.530462384223938**
13      1       [0.666575 0.333425]     **0.666575014591217**
14      0       [0.3897628 0.6102372]   **0.6102372109889984**
15      1       [0.52702403 0.4729759 ]         **0.5270240902900696**
16      1       [0.4889547  0.51104534]         **0.48895466327667236**
17      0       [0.3143539  0.68564606]         **0.6856460869312286**
18      1       [0.5861346 0.4138654]   **0.5861346125602722**
19      1       [0.36064452 0.6393555 ]         0.36064451932907104
20      0       [0.30613276 0.69386727]         **0.6938672363758087**
21      1       [0.5121621 0.4878379]   **0.5121620893478394**
22      1       [0.39942613 0.6005739 ]         0.39942610263824463
23      0       [0.5475982  0.45240185]         **0.4524018168449402**
24      1       [0.32611918 0.67388076]         0.32611924409866333
25      0       [0.44654185 0.55345815]         **0.5534581542015076**
26      1       [0.3096092  0.69039077]         0.30960923433303833
27      1       [0.31329095 0.68670905]         0.31329095363616943
28      0       [0.45154163 0.54845834]         **0.5484583675861359**
29      0       [0.43651828 0.56348175]         **0.5634817183017731**
30      1       [0.30150697 0.69849306]         0.3015069365501404
31      1       [0.65475506 0.34524494]         **0.6547550559043884**
32      1       [0.3094106 0.6905894]   0.30941057205200195
33      0       [0.30295083 0.6970492 ]         **0.6970491707324982**
34      1       [0.44722268 0.5527773 ]         **0.4472227096557617**
35      1       [0.22947736 0.77052265]         0.22947734594345093
36      1       [0.43546116 0.56453884]         **0.435461163520813**
37      1       [0.31734866 0.68265134]         0.3173486590385437
38      1       [0.60735065 0.39264935]         **0.6073506474494934**
39      0       [0.7631628  0.23683721]         0.2368372082710266
40      0       [0.4385739 0.5614261]   **0.5614261031150818**
41      0       [0.74906725 0.25093275]         0.2509327530860901
42      0       [0.5892852 0.4107148]   **0.4107148051261902**
43      0       [0.680065   0.31993505]         0.31993502378463745
44      0       [0.65314734 0.3468526 ]         0.3468526601791382
45      1       [0.3615992 0.6384008]   0.3615992069244385
46      1       [0.32387388 0.6761262 ]         0.3238738179206848
47      0       [0.38049057 0.6195094 ]         **0.6195094287395477**
48      0       [0.49857223 0.50142777]         **0.5014277696609497**
49      1       [0.63154536 0.36845464]         **0.63154536485672**

================

Confusion matrix

================

        P       N
P       18      13
N       15      4