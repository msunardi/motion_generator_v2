_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_5 (GRU)                  (50, 50, 64)              12672     
_________________________________________________________________
gru_6 (GRU)                  (50, 50, 128)             74112     
_________________________________________________________________
gru_7 (GRU)                  (50, 50, 256)             295680    
_________________________________________________________________
dense_3 (Dense)              (50, 50, 2)               514       
=================================================================
Total params: 382,978
Trainable params: 382,978
Non-trainable params: 0
_________________________________________________________________
None
Train on 11450 samples, validate on 1400 samples
Epoch 1/20
11450/11450 [==============================] - 74s 6ms/step - loss: 0.6862 - acc: 0.5637 - val_loss: 0.6847 - val_acc: 0.5665
Epoch 2/20
11450/11450 [==============================] - 73s 6ms/step - loss: 0.6809 - acc: 0.5701 - val_loss: 0.6726 - val_acc: 0.5905
Epoch 3/20
11450/11450 [==============================] - 73s 6ms/step - loss: 0.6678 - acc: 0.5975 - val_loss: 0.6542 - val_acc: 0.6169
Epoch 4/20
11450/11450 [==============================] - 72s 6ms/step - loss: 0.6601 - acc: 0.6122 - val_loss: 0.6468 - val_acc: 0.6322
Epoch 5/20
11450/11450 [==============================] - 72s 6ms/step - loss: 0.6529 - acc: 0.6211 - val_loss: 0.6449 - val_acc: 0.6312
Epoch 6/20
11450/11450 [==============================] - 73s 6ms/step - loss: 0.6477 - acc: 0.6279 - val_loss: 0.6497 - val_acc: 0.6239
Epoch 7/20
11450/11450 [==============================] - 72s 6ms/step - loss: 0.6377 - acc: 0.6372 - val_loss: 0.6776 - val_acc: 0.6176
Epoch 8/20
11450/11450 [==============================] - 73s 6ms/step - loss: 0.5903 - acc: 0.6805 - val_loss: 0.5139 - val_acc: 0.7398
Epoch 9/20
11450/11450 [==============================] - 74s 6ms/step - loss: 0.4781 - acc: 0.7598 - val_loss: 0.3800 - val_acc: 0.8177
Epoch 10/20
11450/11450 [==============================] - 72s 6ms/step - loss: 0.3673 - acc: 0.8236 - val_loss: 0.6948 - val_acc: 0.6877
Epoch 11/20
11450/11450 [==============================] - 73s 6ms/step - loss: 0.3095 - acc: 0.8530 - val_loss: 0.3115 - val_acc: 0.8540
Epoch 12/20
11450/11450 [==============================] - 74s 7ms/step - loss: 0.2781 - acc: 0.8723 - val_loss: 0.2525 - val_acc: 0.8892
Epoch 13/20
11450/11450 [==============================] - 72s 6ms/step - loss: 0.2575 - acc: 0.8819 - val_loss: 0.2409 - val_acc: 0.8951
Epoch 14/20
11450/11450 [==============================] - 72s 6ms/step - loss: 0.2366 - acc: 0.8921 - val_loss: 0.2237 - val_acc: 0.9026
Epoch 15/20
11450/11450 [==============================] - 73s 6ms/step - loss: 0.2265 - acc: 0.8966 - val_loss: 0.2234 - val_acc: 0.9072
Epoch 16/20
11450/11450 [==============================] - 73s 6ms/step - loss: 0.2191 - acc: 0.9002 - val_loss: 0.1965 - val_acc: 0.9153
Epoch 17/20
11450/11450 [==============================] - 73s 6ms/step - loss: 0.2125 - acc: 0.9030 - val_loss: 0.1997 - val_acc: 0.9129
Epoch 18/20
11450/11450 [==============================] - 76s 7ms/step - loss: 0.2106 - acc: 0.9039 - val_loss: 0.2059 - val_acc: 0.9079
Epoch 19/20
11450/11450 [==============================] - 76s 7ms/step - loss: 0.2029 - acc: 0.9078 - val_loss: 0.1991 - val_acc: 0.9154
Epoch 20/20
11450/11450 [==============================] - 77s 7ms/step - loss: 0.1999 - acc: 0.9077 - val_loss: 0.2037 - val_acc: 0.9143
sample  actual          predicted                       prediction error (over correct class)
0       0       [0.99587643 0.00412358]         0.004123568534851074
1       0       [0.9939668  0.00603316]         0.006033182144165039
2       0       [0.9970696  0.00293038]         0.0029304027557373047
3       0       [0.68997735 0.31002268]         0.31002265214920044
4       1       [0.0042115 0.9957885]   0.004211485385894775
5       1       [0.00461602 0.9953839 ]         0.004616081714630127
6       0       [0.9984238  0.00157623]         0.0015761852264404297
7       1       [0.02849818 0.97150177]         0.02849823236465454
8       1       [0.00413835 0.9958617 ]         0.004138290882110596
9       1       [0.00410221 0.9958977 ]         0.00410228967666626
10      1       [0.00149963 0.99850035]         0.0014996528625488281
11      1       [0.00450539 0.9954946 ]         0.0045053958892822266
12      1       [0.00322629 0.99677366]         0.003226339817047119
13      0       [0.9871269  0.01287318]         0.01287311315536499
14      0       [0.9947411  0.00525888]         0.005258917808532715
15      1       [0.00438398 0.9956161 ]         0.0043839216232299805
16      1       [0.00978157 0.99021846]         0.00978153944015503
17      0       [0.59733695 0.40266302]         **0.40266305208206177**
18      1       [0.00979727 0.99020267]         0.009797334671020508
19      0       [0.82013    0.17986993]         0.17987000942230225
20      1       [0.00203177 0.99796826]         0.0020317435264587402
21      1       [0.0015164 0.9984836]   0.0015164017677307129
22      0       [0.98441076 0.0155893 ]         0.015589237213134766
23      1       [0.06720978 0.9327903 ]         0.06720972061157227
24      1       [0.00287495 0.9971251 ]         0.002874910831451416
25      0       [0.62972647 0.3702735 ]         0.37027353048324585
26      1       [0.0026597  0.99734026]         0.002659738063812256
27      0       [0.9949269  0.00507306]         0.005073070526123047
28      1       [0.00305782 0.9969421 ]         0.0030578970909118652
29      1       [0.00278397 0.997216  ]         0.0027840137481689453
30      0       [0.43672574 0.56327426]         **0.5632742643356323**
31      1       [0.00214563 0.9978543 ]         0.002145707607269287
32      0       [0.99345726 0.00654276]         0.0065427422523498535
33      1       [0.00384478 0.9961552 ]         0.0038447976112365723
34      0       [0.99578035 0.0042196 ]         0.004219651222229004
35      0       [0.9974268  0.00257317]         0.0025731921195983887
36      1       [0.00478049 0.9952195 ]         0.004780471324920654
37      0       [0.99789834 0.0021016 ]         0.0021016597747802734
38      1       [0.00701848 0.99298155]         0.007018446922302246
39      0       [0.9942151  0.00578496]         0.005784928798675537
40      1       [0.00879862 0.9912014 ]         0.008798599243164062
41      0       [0.9976776  0.00232244]         0.002322375774383545
42      1       [0.00241648 0.9975835 ]         0.0024164915084838867
43      0       [0.9970878  0.00291228]         0.0029122233390808105
44      1       [0.00244378 0.9975562 ]         0.0024437904357910156
45      0       [0.9820349  0.01796514]         0.017965078353881836
46      1       [0.00237131 0.99762875]         0.0023712515830993652
47      1       [0.00511285 0.9948872 ]         0.005112826824188232
48      0       [0.9972965  0.00270349]         0.0027034878730773926
49      1       [0.00277203 0.99722797]         0.002772033214569092

================

Confusion matrix

================

        P       N
P       28      0
N       2       20
Precision: 0.9333333333333333
Recall: 1.0